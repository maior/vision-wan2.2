# ğŸ“Š í’ˆì§ˆ í‰ê°€ ë©”íŠ¸ë¦­ ìƒì„¸ ì„¤ëª…

## ğŸ“ í•™ê³„ í‘œì¤€ ë©”íŠ¸ë¦­

### 1. CLIP Score (ëª©í‘œ: â‰¥ 0.3)

**ì •ì˜:**
- Text-Video Alignment (í…ìŠ¤íŠ¸-ë¹„ë””ì˜¤ ì˜ë¯¸ì  ì¼ì¹˜ë„)
- OpenAIì˜ CLIP ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìº¡ì…˜ê³¼ ë¹„ë””ì˜¤ í”„ë ˆì„ ê°„ ìœ ì‚¬ë„ ì¸¡ì •

**ê³„ì‚° ë°©ë²•:**
```python
import clip
import torch

model, preprocess = clip.load("ViT-B/32")
frames = [preprocess(f) for f in video_frames]
text = clip.tokenize([caption])

with torch.no_grad():
    frame_features = model.encode_image(frames)
    text_features = model.encode_text(text)

similarity = (frame_features @ text_features.T).mean()
clip_score = similarity.item()
```

**ë²”ìœ„:** 0~1 (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)

**í•´ì„:**
- **0.30+**: ìš°ìˆ˜ (ëª©í‘œ ë‹¬ì„±) âœ…
- **0.25-0.29**: ì–‘í˜¸ (ê°œì„  ê¶Œì¥)
- **0.20-0.24**: ë³´í†µ (ì¬ì‘ì—… í•„ìš”)
- **<0.20**: ë¶ˆëŸ‰ (ì‚¬ìš© ë¶ˆê°€) âŒ

**ì˜ë¯¸:**
- ìº¡ì…˜ì´ ë¹„ë””ì˜¤ ë‚´ìš©ì„ ì–¼ë§ˆë‚˜ ì •í™•í•˜ê²Œ ë¬˜ì‚¬í•˜ëŠ”ì§€ ì •ëŸ‰í™”
- ëª¨ë¸ í•™ìŠµ ì‹œ í…ìŠ¤íŠ¸ ì¡°ê±´ì´ ì˜ í•™ìŠµë  ìˆ˜ ìˆëŠ”ì§€ íŒë‹¨
- CLIP Scoreê°€ ë‚®ìœ¼ë©´ ìƒì„± ëª¨ë¸ì´ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì œëŒ€ë¡œ ë”°ë¥´ì§€ ëª»í•¨

**ì‹¤í–‰ ë°©ë²•:**
```bash
# 1. ì„¤ì¹˜
pip install clip-openai torch torchvision

# 2. ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
python scripts/calculate_clip_score.py \
  --data_path ./preprocessed_data/all_train.csv \
  --sample_size 1000 \
  --model_name ViT-B/32 \
  --output clip_scores.json

# 3. ê²°ê³¼ í™•ì¸
cat clip_scores.json
```

---

### 2. FVD (FrÃ©chet Video Distance, ëª©í‘œ: â‰¤ 1140)

**ì •ì˜:**
- Video Quality Metric (ë¹„ë””ì˜¤ í’ˆì§ˆ ì§€í‘œ)
- ìƒì„±ëœ/ë°ì´í„°ì…‹ ë¹„ë””ì˜¤ì™€ ì‹¤ì œ ë¹„ë””ì˜¤ ê°„ì˜ í†µê³„ì  ê±°ë¦¬ ì¸¡ì •
- FID (FrÃ©chet Inception Distance)ì˜ ë¹„ë””ì˜¤ ë²„ì „

**ê³„ì‚° ë°©ë²•:**
```
FVD = ||Î¼_real - Î¼_gen||Â² + Tr(Î£_real + Î£_gen - 2âˆš(Î£_realÂ·Î£_gen))

ì—¬ê¸°ì„œ:
- Î¼_real: ì‹¤ì œ ë¹„ë””ì˜¤ì˜ I3D íŠ¹ì§• í‰ê· 
- Î¼_gen: ë°ì´í„°ì…‹ ë¹„ë””ì˜¤ì˜ I3D íŠ¹ì§• í‰ê· 
- Î£_real: ì‹¤ì œ ë¹„ë””ì˜¤ì˜ ê³µë¶„ì‚° í–‰ë ¬
- Î£_gen: ë°ì´í„°ì…‹ ë¹„ë””ì˜¤ì˜ ê³µë¶„ì‚° í–‰ë ¬
- Tr: ëŒ€ê°í•© (Trace)
```

**ë²”ìœ„:** 0~âˆ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)

**í•´ì„:**
- **<1140**: ìš°ìˆ˜ (ëª©í‘œ ë‹¬ì„±) âœ…
- **1140-1500**: ì–‘í˜¸ (ê°œì„  ê¶Œì¥)
- **1500-2000**: ë³´í†µ (ì¬ì‘ì—… í•„ìš”)
- **>2000**: ë¶ˆëŸ‰ (ì‚¬ìš© ë¶ˆê°€) âŒ

**ì˜ë¯¸:**
- ë¹„ë””ì˜¤ì˜ ì‹œê°ì  í’ˆì§ˆê³¼ ë‹¤ì–‘ì„±ì„ í‰ê°€
- ë‚®ì€ FVD = ì‹¤ì œ ë¹„ë””ì˜¤ì™€ ìœ ì‚¬í•œ ë¶„í¬ = ê³ í’ˆì§ˆ ë°ì´í„°ì…‹
- ë†’ì€ FVD = ë¹„ì •ìƒì ì´ê±°ë‚˜ ì €í’ˆì§ˆ ë¹„ë””ì˜¤ ë‹¤ìˆ˜ í¬í•¨

**ì‹¤í–‰ ë°©ë²•:**
```bash
# 1. ì„¤ì¹˜
pip install tensorflow tensorflow-gan

# 2. I3D íŠ¹ì§• ì¶”ì¶œ
python scripts/extract_i3d_features.py \
  --real_videos /path/to/reference_videos \
  --dataset_videos /path/to/your_videos \
  --output features.npz

# 3. FVD ê³„ì‚°
python scripts/calculate_fvd.py \
  --features features.npz \
  --output fvd_score.json

# 4. ê²°ê³¼ í™•ì¸
cat fvd_score.json
```

---

## ğŸ¯ í˜„ì¬ ë°ì´í„°ì…‹ ìƒíƒœ

### ì¸¡ì • í•„ìš” í•­ëª©
- âš ï¸ **CLIP Score**: ë¯¸ì¸¡ì • â†’ ìƒ˜í”Œë§í•˜ì—¬ ê³„ì‚° í•„ìš”
- âš ï¸ **FVD**: ë¯¸ì¸¡ì • â†’ Reference ë¹„ë””ì˜¤ì…‹ ì¤€ë¹„ í›„ ê³„ì‚°

### ì¼ê´„ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
```bash
# ëª¨ë“  ë©”íŠ¸ë¦­ í•œ ë²ˆì— ê³„ì‚°
python scripts/evaluate_dataset_quality.py \
  --data_csv ./preprocessed_data/all_train.csv \
  --sample_size 5000 \
  --metrics clip,fvd \
  --output quality_report.json

# ì˜ˆìƒ ì†Œìš” ì‹œê°„: 2-3ì‹œê°„ (V100 GPU í•„ìš”)
```

---

## ğŸ“š ê¸°íƒ€ í’ˆì§ˆ ë©”íŠ¸ë¦­

### ìš°ë¦¬ê°€ í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ë©”íŠ¸ë¦­ (ë°ì´í„°ì…‹ ìˆ˜ì¤€)

**1. ìº¡ì…˜ í’ˆì§ˆ (ê°€ì¤‘ì¹˜ 35%)**
- ê¸¸ì´ ì ì ˆì„±: 20-300ì ê¶Œì¥
- ì–´íœ˜ ë‹¤ì–‘ì„±: TTR (Type-Token Ratio)
- êµ¬ì²´ì„±: êµ¬ì²´ì  ëª…ì‚¬/ë™ì‘ ë¬˜ì‚¬
- ë¬¸ë²• ì •í™•ì„±: ë¬¸ì¥ ì™„ê²°ì„±

**2. ë©”íƒ€ë°ì´í„° ì™„ì „ì„± (ê°€ì¤‘ì¹˜ 25%)**
- í•„ìˆ˜ í•„ë“œ ì™„ì„±ë„: clip_id, media_type, file_path, caption, resolution, length
- ì„ íƒ í•„ë“œ ì™„ì„±ë„: category, keyword

**3. íŒŒì¼ ë¬´ê²°ì„± (ê°€ì¤‘ì¹˜ 25%)**
- íŒŒì¼ ì¡´ì¬ ì—¬ë¶€
- íŒŒì¼ í¬ê¸° ìœ íš¨ì„±
- íŒŒì¼ ì½ê¸° ê°€ëŠ¥ ì—¬ë¶€

**4. ë°ì´í„° ê· í˜•ì„± (ê°€ì¤‘ì¹˜ 15%)**
- ì¹´í…Œê³ ë¦¬ ë¶„í¬ Entropy
- í•´ìƒë„ ë¶„í¬ ê· í˜•
- ë¹„ë””ì˜¤ ê¸¸ì´ ë¶„ì‚°

**í˜„ì¬ ì¢…í•© ì ìˆ˜: 91.8%** (Aë“±ê¸‰)

---

## ğŸš€ í’ˆì§ˆ í–¥ìƒ ë¡œë“œë§µ

### Phase 1: í•™ê³„ í‘œì¤€ ë©”íŠ¸ë¦­ ì¸¡ì • (1ì£¼)
1. CLIP Score ê³„ì‚° ìŠ¤í¬ë¦½íŠ¸ ì‘ì„± ë° ì‹¤í–‰
2. Reference ë¹„ë””ì˜¤ì…‹ ì¤€ë¹„ (ì˜ˆ: Kinetics-700, WebVid)
3. FVD ê³„ì‚° íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
4. ê²°ê³¼ ëŒ€ì‹œë³´ë“œì— í†µí•©

**ëª©í‘œ:**
- CLIP Score â‰¥ 0.3
- FVD â‰¤ 1140

### Phase 2: ìº¡ì…˜ í’ˆì§ˆ ê³ ë„í™” (2ì£¼)
1. KoNLPy/Mecab í˜•íƒœì†Œ ë¶„ì„ í†µí•©
2. LLM ê¸°ë°˜ ìº¡ì…˜ í’ˆì§ˆ ìë™ í‰ê°€ (GPT-4, Claude)
3. ì €í’ˆì§ˆ ìº¡ì…˜ ì¬ì‘ì„± íŒŒì´í”„ë¼ì¸
4. ìº¡ì…˜ ê°€ì´ë“œë¼ì¸ ë¬¸ì„œí™”

**ì˜ˆìƒ íš¨ê³¼:** CLIP Score +0.05~0.08

### Phase 3: íŒŒì¼ ë¬´ê²°ì„± ìë™í™” (1ì£¼)
1. ffprobe ê¸°ë°˜ ë¹„ë””ì˜¤ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
2. ì†ìƒ íŒŒì¼ ìë™ íƒì§€
3. íŒŒì¼ í•´ì‹œ DB êµ¬ì¶•
4. ì£¼ê¸°ì  ê²€ì¦ í¬ë¡ ì¡

**ì˜ˆìƒ íš¨ê³¼:** FVD -50~100

### Phase 4: ë°ì´í„° ì¦ê°• ë° ê· í˜• ì¡°ì • (2ì£¼)
1. ê³¼ì†Œí‘œ ì¹´í…Œê³ ë¦¬ ì¦ê°•
2. í•´ìƒë„ ì •ê·œí™” (1280Ã—720, 1920Ã—1080 â†’ Wan2.2 ì§€ì› í•´ìƒë„)
3. Train/Val/Test ë¶„í•  ì¬ì¡°ì •

**ì˜ˆìƒ íš¨ê³¼:** ì „ì²´ í’ˆì§ˆ +2~3%

---

## ğŸ“– ì°¸ê³  ë¬¸í—Œ

1. **CLIP: Learning Transferable Visual Models From Natural Language Supervision**
   - Radford et al., OpenAI, 2021
   - https://arxiv.org/abs/2103.00020

2. **Towards Accurate Generative Models of Video: A New Metric & Challenges**
   - Unterthiner et al., DeepMind, 2018
   - FVD ë©”íŠ¸ë¦­ ì†Œê°œ
   - https://arxiv.org/abs/1812.01717

3. **GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium**
   - Heusel et al., 2017
   - FID (FVDì˜ ê¸°ë°˜) ë©”íŠ¸ë¦­
   - https://arxiv.org/abs/1706.08500

4. **I3D: Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset**
   - Carreira & Zisserman, Google DeepMind, 2017
   - FVD ê³„ì‚°ì— ì‚¬ìš©ë˜ëŠ” I3D íŠ¹ì§• ì¶”ì¶œê¸°
   - https://arxiv.org/abs/1705.07750

---

## ğŸ’¡ ì‹¤ë¬´ íŒ

### CLIP Score ìµœì í™”
- **ìº¡ì…˜ êµ¬ì²´í™”**: "ì‚¬ëŒì´ ê±·ëŠ”ë‹¤" â†’ "í•œ ë‚¨ì„±ì´ ê³µì› ì‚°ì±…ë¡œë¥¼ ì—¬ìœ ë¡­ê²Œ ê±·ê³  ìˆë‹¤"
- **ì‹œê°„ì  ë¬˜ì‚¬**: "ì°¨ê°€ ë‹¬ë¦°ë‹¤" â†’ "ë¹¨ê°„ ìŠ¤í¬ì¸ ì¹´ê°€ ê³ ì†ë„ë¡œë¥¼ ë¹ ë¥´ê²Œ ì§ˆì£¼í•œë‹¤"
- **ë°°ê²½ í¬í•¨**: "ê³ ì–‘ì´ê°€ ë…¼ë‹¤" â†’ "íšŒìƒ‰ ê³ ì–‘ì´ê°€ ê±°ì‹¤ ì†ŒíŒŒì—ì„œ í„¸ì‹¤ì„ ê°€ì§€ê³  ë…¼ë‹¤"

### FVD ìµœì í™”
- **í•´ìƒë„ ì¼ê´€ì„±**: ëª¨ë“  ë¹„ë””ì˜¤ë¥¼ ë™ì¼ í•´ìƒë„ë¡œ ì •ê·œí™”
- **í”„ë ˆì„ë ˆì´íŠ¸ í†µì¼**: 30fps ë˜ëŠ” 60fpsë¡œ í†µì¼
- **ì†ìƒ íŒŒì¼ ì œê±°**: ê¹¨ì§„ í”„ë ˆì„, ê³¼ë„í•œ ì••ì¶•, ì•„í‹°íŒ©íŠ¸ ì œê±°
- **ì¹´ë©”ë¼ ì›€ì§ì„ ê· í˜•**: ì •ì /ë™ì  ë¹„ë””ì˜¤ ì ì ˆíˆ í˜¼í•©

### Reference ë¹„ë””ì˜¤ì…‹ ì„ íƒ
- **Kinetics-700**: ì•¡ì…˜ ì¸ì‹ ë°ì´í„°ì…‹, ë‹¤ì–‘í•œ ë™ì‘
- **WebVid-10M**: í…ìŠ¤íŠ¸-ë¹„ë””ì˜¤ ìŒ, ì¼ìƒì  ì¥ë©´
- **UCF-101**: ì•¡ì…˜ í´ë¦½, ì§§ì€ ë™ì‘ ì¤‘ì‹¬
- **ë‚´ë¶€ ê³ í’ˆì§ˆ ì…‹**: ì§ì ‘ ì´¬ì˜í•œ ê³ í•´ìƒë„ ë¹„ë””ì˜¤ (ì¶”ì²œ)

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

### ë°ì´í„°ì…‹ í’ˆì§ˆ í™•ì¸
- [ ] CLIP Score â‰¥ 0.3
- [ ] FVD â‰¤ 1140
- [ ] ìº¡ì…˜ í’ˆì§ˆ â‰¥ 90%
- [ ] ë©”íƒ€ë°ì´í„° ì™„ì„±ë„ â‰¥ 95%
- [ ] íŒŒì¼ ë¬´ê²°ì„± â‰¥ 99%
- [ ] ì¹´í…Œê³ ë¦¬ ê· í˜• (Entropy > 3.0)

### ìŠ¤í¬ë¦½íŠ¸ ì¤€ë¹„
- [ ] `scripts/calculate_clip_score.py`
- [ ] `scripts/extract_i3d_features.py`
- [ ] `scripts/calculate_fvd.py`
- [ ] `scripts/evaluate_dataset_quality.py`

### ì¸í”„ë¼
- [ ] GPU í™˜ê²½ (V100 Ã— 2 ì´ìƒ)
- [ ] Reference ë¹„ë””ì˜¤ì…‹ ì¤€ë¹„
- [ ] ì¶©ë¶„í•œ ë””ìŠ¤í¬ ê³µê°„ (íŠ¹ì§• ì¶”ì¶œìš©)

---

**ëŒ€ì‹œë³´ë“œì—ì„œ í™•ì¸:** http://211.180.253.250:7020/quality/methodology
