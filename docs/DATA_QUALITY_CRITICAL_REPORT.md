# 🚨 데이터 품질 중요 발견 보고서

생성일: 2025-11-08
분석 대상: MBC 데이터셋 전처리 후 품질 검증

---

## ⚠️ 핵심 발견: 검증 데이터의 94%가 사용 불가

### 현재 상황 요약

| 데이터셋 | 총 샘플 | 정상 샘플 | 결함 샘플 | 결함률 | 상태 |
|----------|---------|-----------|-----------|--------|------|
| **학습 데이터** | 170,180 | 167,906 | 2,274 | **1.34%** | ✅ 양호 |
| **검증 데이터** | 20,000 | 1,199 | 18,801 | **94.01%** | ❌ 심각 |

---

## 📊 상세 분석

### 1. 학습 데이터 (all_train.csv)

#### ✅ **상태: 양호**

- **총 샘플**: 170,180개
- **정상 샘플**: 167,906개 (98.66%)
- **결함 샘플**: 2,274개 (1.34%)

#### 결함 유형 분석

| 결함 유형 | 개수 | 설명 |
|-----------|------|------|
| `too_long` | 1,726 | Caption 1,500자 초과 또는 비디오 45초 초과 |
| `repetition` | 602 | 같은 단어 10번 이상 반복 (복붙 의심) |
| `char_repetition` | 9 | 같은 문자 15번 이상 반복 |

#### 결함 샘플 예시 (상위 5개)

1. **Clip ID: 2434304** (3개 이슈)
   - Caption 길이: 2,874자 → **제외 기준: 1,500자 초과**
   - 비디오 길이: 642.5초 (10분 42초) → **제외 기준: 45초 초과**
   - 문자 반복 패턴 발견

2. **Clip ID: 3332996** (3개 이슈)
   - Caption 길이: **10,766자** (최대값!)
   - 비디오 길이: 1,746초 (29분)
   - "있습니다" 단어 25번 반복

3. **Clip ID: 2432976** (3개 이슈)
   - Caption 길이: 1,513자
   - 비디오 길이: 253.7초
   - "하하하하" 단어 **39번 반복** (명백한 오류)

#### 권장사항
✅ **사용 가능**: `data_quality_analysis/clean_dataset.csv` (167,906개)

---

### 2. 검증 데이터 (all_val.csv)

#### 🚨 **상태: 심각한 문제**

- **총 샘플**: 20,000개
- **정상 샘플**: 1,199개 (6.00%)
- **결함 샘플**: 18,801개 (94.01%) ← **심각!**

#### 결함 유형 분석

| 결함 유형 | 개수 | 설명 |
|-----------|------|------|
| **`unsupported_resolution`** | **18,761** | **1920×1080 등 Wan2.2 미지원 해상도** |
| `too_long` | 230 | Caption 또는 비디오가 너무 긺 |
| `repetition` | 75 | 단어 반복 |

#### 🔴 핵심 문제: 해상도 변환 미완료

**검증 데이터의 93.81%가 원본 해상도 그대로!**

```
1920×1080: 15,490개 (77.45%)  ← Wan2.2 미지원
720×512:    3,096개 (15.48%)  ← Wan2.2 미지원
1280×720:   1,239개 (6.19%)   ← Wan2.2 지원 ✓
기타:         175개 (0.88%)   ← Wan2.2 미지원
```

#### 결함 샘플 예시

1. **Clip ID: 3247504** (4개 이슈)
   - Caption: 2,921자
   - 비디오: 553.8초
   - 해상도: **1920×1080** ← 변환 필요!

2. **Clip ID: 2921445** (4개 이슈)
   - Caption: 3,239자
   - 비디오: 426.9초
   - 해상도: **640×640** ← 변환 필요!

---

## 🎯 결함 데이터 제외 기준

### 설정된 필터링 기준 (연구 기반)

| 항목 | 최소값 | 최대값 | 이유 |
|------|--------|--------|------|
| **Caption 길이** | 50자 | 1,500자 | 너무 짧거나 긴 caption은 노이즈 |
| **비디오 길이** | 5초 | 45초 | Wan2.2 학습에 적절한 범위 |
| **해상도** | - | - | Wan2.2 지원 해상도만 |

### Wan2.2 지원 해상도

```
✓ 1280×720, 720×1280
✓ 832×480, 480×832
✓ 1280×704, 704×1280
✓ 1024×704, 704×1024
```

### 품질 패턴 검사

- ✅ 같은 단어 10번 이상 반복 → 제외
- ✅ 같은 문자 15번 이상 반복 → 제외
- ✅ 단어 수 15개 미만 → 제외

---

## 🔧 해결 방안

### 옵션 1: 검증 데이터 전처리 (권장)

**검증 데이터도 1280×720으로 변환 필요**

```bash
# 검증 데이터 해상도 변환
python preprocess_mbc_data.py \
  --mode validation \
  --input ./preprocessed_data/all_val.csv \
  --output ./preprocessed_data/all_val_720p.csv \
  --target_resolution 1280 720
```

예상 결과:
- 변환 후: 약 20,000개 → 18,000개 (변환 성공률 90% 가정)
- 품질 필터링 후: 약 17,700개 사용 가능

---

### 옵션 2: 클린 데이터로 재분할 (대안)

**정상 데이터만 사용하여 새로운 train/val split**

```python
# 클린 데이터 167,906개를 90:10으로 재분할
train: 151,115개 (90%)
val:    16,791개 (10%)
```

장점:
- 모든 데이터가 품질 검증 통과
- 해상도 문제 없음
- 즉시 학습 시작 가능

단점:
- 기존 검증 세트와 다름 (재현성 문제)

---

### 옵션 3: 소규모 검증 세트 (비권장)

**현재 정상 데이터 1,199개만 사용**

- 검증 세트가 너무 작음 (학습 데이터의 0.71%)
- 통계적 신뢰도 낮음
- ❌ 권장하지 않음

---

## 📋 데이터 정제 결과 파일

### 생성된 파일

#### 학습 데이터
```
./data_quality_analysis/
├── defective_samples.csv    # 결함 샘플 2,274개 (참고용)
└── clean_dataset.csv         # 정상 샘플 167,906개 ← 사용 가능!
```

#### 검증 데이터
```
./data_quality_analysis_val/
├── defective_samples.csv    # 결함 샘플 18,801개
└── clean_dataset.csv         # 정상 샘플 1,199개 (너무 적음)
```

---

## 🎬 권장 조치

### 우선순위 1: 검증 데이터 전처리 (필수!)

```bash
# 1. 검증 데이터 해상도 변환
python preprocess_validation_data.py

# 2. 품질 검증 및 필터링
python analyze_defective_data.py \
  --csv_path ./preprocessed_data/all_val_converted.csv \
  --output_dir ./data_quality_analysis_val_clean

# 3. 결과 확인
# - 예상: 17,700개 정상 샘플 (88.5%)
```

---

### 우선순위 2: 클린 데이터셋으로 학습 시작

**즉시 사용 가능한 데이터**

```python
# 학습 설정 업데이트
train_csv = "./data_quality_analysis/clean_dataset.csv"  # 167,906개

# 검증 데이터 선택지:
# A. 검증 데이터 전처리 완료 후
val_csv = "./data_quality_analysis_val_clean/clean_dataset.csv"  # 17,700개

# B. 클린 데이터 재분할 (대안)
# 167,906개를 90:10으로 split
# train: 151,115개
# val:    16,791개
```

---

## 📊 예상 최종 데이터셋

### 옵션 1: 검증 데이터 전처리 후

| 데이터셋 | 원본 | 결함 제외 | 최종 | 비율 |
|----------|------|-----------|------|------|
| Train | 170,180 | -2,274 | **167,906** | 90.4% |
| Val | 20,000 | -2,300 | **17,700** | 9.6% |
| **Total** | 190,180 | -4,574 | **185,606** | 100% |

---

### 옵션 2: 클린 데이터 재분할

| 데이터셋 | 샘플 수 | 비율 |
|----------|---------|------|
| Train | **151,115** | 90% |
| Val | **16,791** | 10% |
| **Total** | **167,906** | 100% |

---

## 🚨 중요 경고

### ❌ 절대 하면 안 되는 것

1. **검증 데이터 18,801개를 그대로 사용**
   - 94%가 결함 데이터
   - 해상도 미지원으로 학습 불가
   - 평가 결과 신뢰 불가

2. **결함 데이터 포함하여 학습**
   - 너무 긴 caption → 메모리 부족
   - 너무 긴 비디오 → GPU OOM
   - 반복 패턴 → 모델 품질 저하

3. **1,199개만으로 검증**
   - 너무 작은 검증 세트
   - 통계적 신뢰도 매우 낮음

---

## ✅ 최종 권장사항

### 단기 (즉시)

1. ✅ **학습 데이터**: `data_quality_analysis/clean_dataset.csv` 사용
   - 167,906개 정상 샘플
   - 품질 검증 완료
   - 해상도 1280×720 변환 완료

2. ⚠️ **검증 데이터**: 검증 데이터 전처리 스크립트 작성 및 실행 필요
   - 현재 94%가 사용 불가 상태

---

### 중기 (1-2일 내)

1. 🔧 검증 데이터 해상도 변환 스크립트 작성
2. 🔧 품질 필터링 재실행
3. ✅ 최종 클린 검증 데이터셋 생성 (~17,700개)

---

### 대안 (즉시 학습 시작 원할 경우)

**옵션 2 선택: 클린 데이터 재분할**

```python
# 스크립트 작성: split_clean_data.py
import pandas as pd
from sklearn.model_selection import train_test_split

df = pd.read_csv('data_quality_analysis/clean_dataset.csv')

train, val = train_test_split(
    df,
    test_size=0.1,
    random_state=42,
    stratify=df['media_type']  # 미디어 타입 균형 유지
)

train.to_csv('final_train.csv', index=False)
val.to_csv('final_val.csv', index=False)

print(f"Train: {len(train):,}")
print(f"Val: {len(val):,}")
```

---

## 📈 데이터 정제 효과 예측

### 학습 품질 개선 예상

| 항목 | 정제 전 | 정제 후 | 개선 |
|------|---------|---------|------|
| Caption 품질 | 혼재 | 일관됨 | ↑ 모델 수렴 속도 |
| 비디오 길이 | 3초~40분 | 5초~45초 | ↑ 메모리 안정성 |
| 해상도 | 혼재 | 1280×720 | ✓ 학습 가능 |
| 반복 패턴 | 포함 | 제거 | ↑ 생성 품질 |

### 메모리 사용량 예측

```
정제 전:
- 최대 Caption: 10,766자 → 메모리 폭발 위험!
- 최대 비디오: 2,383초 → GPU OOM 확실!

정제 후:
- 최대 Caption: 1,500자 → 안전
- 최대 비디오: 45초 → V100 32GB에서 안정적
```

---

## 🎯 결론

**사용자의 지적이 100% 정확합니다!**

> "결국 전체 데이터를 가지고 학습할것이 아니라 결함치를 만들고 이것들은 제외하고 전처리가 되어야지?"

✅ **맞습니다!** 반드시 결함 데이터를 제외하고 학습해야 합니다.

### 현재 상황
- 학습 데이터: **98.66% 정상** ✅
- 검증 데이터: **6% 정상** ❌ ← 심각한 문제!

### 조치 필요
1. **즉시**: 학습 데이터 → `clean_dataset.csv` 사용
2. **필수**: 검증 데이터 전처리 (해상도 변환)
3. **권장**: 품질 필터링 후 최종 데이터셋 생성

### 학습 시작 조건
- ✅ 학습 데이터 준비 완료
- ❌ 검증 데이터 전처리 필요
- **예상 소요 시간**: 해상도 변환 1-2시간

---

**보고서 끝**

**다음 단계**: 검증 데이터 전처리 스크립트 작성 필요!
