# Wan2.2 TI2V-5B LoRA íŒŒì¸íŠœë‹ êµ¬í˜„ ì™„ë£Œ

## âœ… êµ¬í˜„ ì™„ë£Œ ì‚¬í•­

### 1. ë°ì´í„°ì…‹ ë¶„ì„ ë° ê³„íš âœ…
- **ìœ„ì¹˜**: `FINETUNING_PLAN.md`
- **ë‚´ìš©**:
  - 455,648ê°œ JSON íŒŒì¼ ë¶„ì„
  - ë°ì´í„° êµ¬ì¡° íŒŒì•… (ì´ë¯¸ì§€ 1ê°œ, ë¹„ë””ì˜¤ 1ê°œ í™•ì¸)
  - í•™ìŠµ ì „ëµ ìˆ˜ë¦½
  - ë©”ëª¨ë¦¬ ìµœì í™” ê³„íš

### 2. ë°ì´í„° ë¡œë” êµ¬í˜„ âœ…
- **íŒŒì¼**: `lora_finetuning/data/dataset.py`
- **ê¸°ëŠ¥**:
  - JSON íŒŒì‹± ë° í•„í„°ë§ (ë¹„ë””ì˜¤ë§Œ, 3-30ì´ˆ)
  - CloudFront CDNì—ì„œ ë¹„ë””ì˜¤/ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
  - ë‹¤ë‹¨ê³„ ìº¡ì…˜ í†µí•© (object, semantic, application level)
  - T2V/I2V í˜¼í•© ëª¨ë“œ ì§€ì›
  - ì˜¨ë””ë§¨ë“œ ìºì‹±

### 3. LoRA ëª¨ë“ˆ êµ¬í˜„ âœ…
- **íŒŒì¼**: `lora_finetuning/models/lora_layers.py`
- **ë‚´ìš©**:
  - `LoRALinear`: Low-rank matrix êµ¬í˜„
  - `LoRALayer`: ê¸°ì¡´ Linear layerì— LoRA ì¶”ê°€
  - Parameter freeze ë° ê´€ë¦¬ í•¨ìˆ˜

### 4. LoRA ì£¼ì… ì½”ë“œ âœ…
- **íŒŒì¼**: `lora_finetuning/models/inject_lora.py`
- **ê¸°ëŠ¥**:
  - DiT ëª¨ë¸ì˜ attention layersì— LoRA ì£¼ì…
  - Self-attention ë° Cross-attention íƒ€ê²ŸíŒ…
  - LoRA weights ì €ì¥/ë¡œë“œ
  - Merge/unmerge ê¸°ëŠ¥ (ì¶”ë¡  ìµœì í™”)

### 5. í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ âœ…
- **íŒŒì¼**: `lora_finetuning/training/train_lora.py`
- **ê¸°ëŠ¥**:
  - ë¶„ì‚° í•™ìŠµ (DDP, FSDP, Ulysses)
  - Diffusion loss ê³„ì‚°
  - Mixed precision (BF16)
  - Gradient checkpointing
  - 8bit AdamW optimizer
  - LR warmup scheduler
  - ì²´í¬í¬ì¸íŠ¸ ì €ì¥

### 6. ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ âœ…
- **íŒŒì¼**: `lora_finetuning/scripts/train.sh`
- **ì„¤ì •**:
  - V100 32GB Ã— 2 ìµœì í™”
  - ëª¨ë“  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
  - ë¡œê¹… ë° ì²´í¬í¬ì¸íŒ…

### 7. í™˜ê²½ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸ âœ…
- **íŒŒì¼**: `lora_finetuning/scripts/test_env.py`
- **ê²€ì‚¬ í•­ëª©**:
  - ëª¨ë“  dependencies
  - GPU ë©”ëª¨ë¦¬
  - ë°ì´í„° ë¡œë”©
  - LoRA ì£¼ì…

### 8. ë¬¸ì„œí™” âœ…
- **íŒŒì¼**:
  - `lora_finetuning/README.md`: ì‚¬ìš© ê°€ì´ë“œ
  - `lora_finetuning/configs/default_config.yaml`: ì„¤ì • ì˜ˆì‹œ
  - `FINETUNING_PLAN.md`: ìƒì„¸ ê³„íš

---

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
Wan2.2/
â”œâ”€â”€ lora_finetuning/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ dataset.py           # MBC ë°ì´í„°ì…‹
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ lora_layers.py       # LoRA êµ¬í˜„
â”‚   â”‚   â”œâ”€â”€ inject_lora.py       # LoRA ì£¼ì…
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â””â”€â”€ train_lora.py        # ë©”ì¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”œâ”€â”€ train.sh             # í•™ìŠµ ì‹¤í–‰
â”‚   â”‚   â””â”€â”€ test_env.py          # í™˜ê²½ ê²€ì¦
â”‚   â”œâ”€â”€ configs/
â”‚   â”‚   â””â”€â”€ default_config.yaml  # ê¸°ë³¸ ì„¤ì •
â”‚   â”œâ”€â”€ checkpoints/             # LoRA weights
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ FINETUNING_PLAN.md
â””â”€â”€ LORA_IMPLEMENTATION_SUMMARY.md
```

---

## ğŸš€ ì‚¬ìš© ë°©ë²•

### 1. í™˜ê²½ ê²€ì¦

```bash
python lora_finetuning/scripts/test_env.py
```

### 2. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

```bash
huggingface-cli download Wan-AI/Wan2.2-TI2V-5B --local-dir ./Wan2.2-TI2V-5B
```

### 3. ì„¤ì • ìˆ˜ì •

`lora_finetuning/scripts/train.sh`ì—ì„œ:
```bash
MODEL_DIR="/path/to/Wan2.2-TI2V-5B"  # ê²½ë¡œ ìˆ˜ì •
```

### 4. í•™ìŠµ ì‹¤í–‰

```bash
# ì „ì²´ í•™ìŠµ
bash lora_finetuning/scripts/train.sh

# ë””ë²„ê·¸ ëª¨ë“œ
# train.shì—ì„œ MAX_SAMPLES ì£¼ì„ í•´ì œ í›„
bash lora_finetuning/scripts/train.sh
```

### 5. ì¶”ë¡ 

```python
from wan.textimage2video import WanTI2V
from wan.configs import ti2v_5B
from lora_finetuning.models import inject_lora_to_dit_model, load_lora_weights

# Load model + LoRA
model = WanTI2V(config=ti2v_5B, checkpoint_dir='./Wan2.2-TI2V-5B')
model.model = inject_lora_to_dit_model(model.model, r=16, alpha=16)
load_lora_weights(model.model, 'outputs/[timestamp]/lora_final.pth')

# Generate
video = model.generate(prompt="ë‰´ìŠ¤ ì•µì»¤ê°€ ë‰´ìŠ¤ë¥¼ ì „í•˜ëŠ” ì¥ë©´", num_frames=49)
```

---

## ğŸ“Š í•µì‹¬ ì„¤ì •

| í•­ëª© | ê°’ | ì„¤ëª… |
|-----|-----|------|
| **í•˜ë“œì›¨ì–´** | V100 32GB Ã— 2 | GPU ìš”êµ¬ì‚¬í•­ |
| **LoRA Rank** | 16 | Low-rank ì°¨ì› |
| **Batch Size** | 1 Ã— 2 Ã— 8 = 16 | Effective batch size |
| **Learning Rate** | 1e-4 | ì´ˆê¸° í•™ìŠµë¥  |
| **Max Steps** | 10,000 | ì´ í•™ìŠµ ìŠ¤í… |
| **Frames** | 49 | í”„ë ˆì„ ìˆ˜ (ë©”ëª¨ë¦¬ ê³ ë ¤) |
| **Resolution** | 1280Ã—704 | TI2V-5B í‘œì¤€ |
| **ë°ì´í„°** | 455,648 JSON | ì „ì²´ ë°ì´í„°ì…‹ |

---

## ğŸ”§ ë©”ëª¨ë¦¬ ìµœì í™”

V100 32GB Ã— 2ì—ì„œ í•™ìŠµ ê°€ëŠ¥í•˜ë„ë¡ ë‹¤ìŒ ê¸°ë²• ì ìš©:

1. âœ… **FSDP** (Fully Sharded Data Parallel)
2. âœ… **DeepSpeed Ulysses** (Sequence parallelism)
3. âœ… **Gradient Checkpointing**
4. âœ… **Mixed Precision** (BF16)
5. âœ… **T5 CPU Offloading**
6. âœ… **8bit AdamW** Optimizer
7. âœ… **Frame ìˆ˜ ì¡°ì •** (81 â†’ 49)

**ì˜ˆìƒ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: ~31GB per GPU âœ…

---

## ğŸ“ˆ ì˜ˆìƒ ê²°ê³¼

- **í•™ìŠµ ì‹œê°„**: ~58ì¼ (10,000 steps)
- **LoRA íŒŒë¼ë¯¸í„°**: ~500MB
- **ì´ íŒŒë¼ë¯¸í„°**: ~5B (frozen) + ~500MB (trainable)
- **í•™ìŠµ ìƒ˜í”Œ**: ~160,000 (ì „ì²´ì˜ 35%)

### ì†ë„ ê°œì„  ì˜µì…˜

í”„ë ˆì„ ìˆ˜ë¥¼ ë” ì¤„ì´ë©´ í•™ìŠµ ì†ë„ í–¥ìƒ:
- 49 frames â†’ 33 frames: ~40% ë¹ ë¦„
- í•´ìƒë„ ë‚®ì¶”ê¸°: 1280Ã—704 â†’ 704Ã—1280

---

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

### 1. í™˜ê²½ ì¤€ë¹„
```bash
# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt
pip install bitsandbytes

# í™˜ê²½ ê²€ì¦
python lora_finetuning/scripts/test_env.py
```

### 2. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
```bash
huggingface-cli download Wan-AI/Wan2.2-TI2V-5B --local-dir ./Wan2.2-TI2V-5B
```

### 3. ë””ë²„ê·¸ í•™ìŠµ
```bash
# train.sh ìˆ˜ì •:
# MAX_SAMPLES="--max_samples 100"

bash lora_finetuning/scripts/train.sh
```

### 4. ì „ì²´ í•™ìŠµ
```bash
# train.shì—ì„œ MAX_SAMPLES ì£¼ì„ ì²˜ë¦¬
bash lora_finetuning/scripts/train.sh
```

---

## ğŸ› íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### OOM (Out of Memory)
```bash
# Option 1: Frame ìˆ˜ ì¤„ì´ê¸°
NUM_FRAMES=33  # 49 â†’ 33

# Option 2: Gradient accumulation ì¦ê°€
GRAD_ACCUM=16  # 8 â†’ 16

# Option 3: í•´ìƒë„ ë‚®ì¶”ê¸°
SIZE="704*1280"  # 1280*704 â†’ 704*1280
```

### ëŠë¦° ë‹¤ìš´ë¡œë“œ
```bash
# DataLoader workers ì¦ê°€
NUM_WORKERS=8
```

### CUDA Error
```bash
# GPU ë¦¬ì…‹
nvidia-smi --gpu-reset

# PyTorch ì¬ì„¤ì¹˜
pip install --upgrade --force-reinstall torch torchvision
```

---

## ğŸ“š ì°¸ê³  ìë£Œ

### ë…¼ë¬¸
- [Wan2.2 Paper](https://arxiv.org/abs/2503.20314)
- [LoRA Paper](https://arxiv.org/abs/2106.09685)
- [FSDP](https://arxiv.org/abs/2304.11277)
- [DeepSpeed Ulysses](https://arxiv.org/abs/2309.14509)

### ë¬¸ì„œ
- [PyTorch FSDP Guide](https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html)
- [HuggingFace LoRA](https://huggingface.co/docs/peft/conceptual_guides/lora)
- [bitsandbytes 8bit](https://github.com/TimDettmers/bitsandbytes)

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

### êµ¬í˜„ ì™„ë£Œ
- [x] ë°ì´í„°ì…‹ ë¶„ì„
- [x] ë°ì´í„° ë¡œë”
- [x] LoRA ëª¨ë“ˆ
- [x] LoRA ì£¼ì…
- [x] í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
- [x] ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
- [x] í™˜ê²½ ê²€ì¦
- [x] ë¬¸ì„œí™”

### ì‹¤í–‰ ì¤€ë¹„
- [ ] í™˜ê²½ ê²€ì¦ (`test_env.py`)
- [ ] ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (TI2V-5B)
- [ ] ê²½ë¡œ ì„¤ì • (`train.sh`)
- [ ] ë””ë²„ê·¸ í•™ìŠµ (100 samples)
- [ ] ì „ì²´ í•™ìŠµ ì‹¤í–‰

---

## ğŸ’¡ í•µì‹¬ í¬ì¸íŠ¸

1. **V100 32GB Ã— 2ë¡œ í•™ìŠµ ê°€ëŠ¥** âœ…
   - TI2V-5BëŠ” A14Bë³´ë‹¤ 3ë°° ì‘ìŒ
   - ì ì ˆí•œ ë©”ëª¨ë¦¬ ìµœì í™”ë¡œ ì‹¤í–‰ ê°€ëŠ¥

2. **MBC ë°ì´í„° í™œìš©** âœ…
   - 455,648ê°œ JSON íŒŒì¼
   - ë‹¤ë‹¨ê³„ ìº¡ì…˜ (object, semantic, application)
   - STT ìŠ¤í¬ë¦½íŠ¸ í†µí•©

3. **LoRAë¡œ íš¨ìœ¨ì  íŒŒì¸íŠœë‹** âœ…
   - ì „ì²´ ëª¨ë¸ì˜ ~1% íŒŒë¼ë¯¸í„°ë§Œ í•™ìŠµ
   - ~500MB LoRA weights
   - ë¹ ë¥¸ í•™ìŠµ ë° ë°°í¬

4. **í”„ë¡œë•ì…˜ ready** âœ…
   - ë¶„ì‚° í•™ìŠµ ì§€ì›
   - ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬
   - ì—ëŸ¬ í•¸ë“¤ë§
   - ìƒì„¸í•œ ë¬¸ì„œí™”

---

## ğŸ“§ ë¬¸ì˜

êµ¬í˜„ ê´€ë ¨ ì§ˆë¬¸ì´ë‚˜ ë¬¸ì œê°€ ìˆìœ¼ë©´ ì•Œë ¤ì£¼ì„¸ìš”!
