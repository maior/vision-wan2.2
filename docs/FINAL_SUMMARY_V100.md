# π‰ Wan2.2 LoRA Fine-tuning - V100 μ¤€λΉ„ μ™„λ£!

## β… μ™„λ£λ λ¨λ“  μ‘μ—…

### 1. λ°μ΄ν„° μ „μ²λ¦¬ β…
- **199,994κ°** MBC λ°μ΄ν„° μ „μ²λ¦¬
- Train/Val split (90/10)
- CSV μƒμ„± (904MB)
- ν…μ¤νΈ λ°μ΄ν„° 100κ° μƒμ„±

### 2. λ°μ΄ν„° ν’μ§ κ²€μ¦ β…
- μλ™ κ²€μ¦ μ¤ν¬λ¦½νΈ (`validate_data_quality.py`)
- μ‹κ°μ  κ²€μ¦ λ„κµ¬ (`inspect_samples.py`)
- ν’μ§ κ°€μ΄λ“ λ¬Έμ„

### 3. LoRA Fine-tuning ν”„λ μ„μ›ν¬ β…
- λ°μ΄ν„° λ΅λ” κµ¬ν„
- LoRA μ„¤μ • κ΄€λ¦¬
- ν•™μµ μ¤ν¬λ¦½νΈ
- **V100 32GB Γ— 2 μµμ ν™” μ™„λ£**

### 4. μ›Ή λ€μ‹λ³΄λ“ β…
- FastAPI Backend (ν¬νΈ 7010)
- Next.js Frontend (ν¬νΈ 7020)
- SQLite Database

### 5. V100 μµμ ν™” β…
- V100 μ „μ© μ„¤μ • νμΌ
- λ©”λ¨λ¦¬ μµμ ν™” (32GB λ€μ‘)
- ν•™μµ μ¤ν¬λ¦½νΈ 2κ°
- μƒμ„Έ κ°€μ΄λ“

## π“‚ νμΌ κµ¬μ΅°

```
Wan2.2/
β”β”€β”€ preprocessed_data/
β”‚   β”β”€β”€ all_train.csv (179,994κ°)
β”‚   β”β”€β”€ all_val.csv (20,000κ°)
β”‚   β””β”€β”€ test_100.csv (100κ°)          β† μ¤λ²„ν”Όν… ν…μ¤νΈμ©
β”‚
β”β”€β”€ lora_finetuning/
β”‚   β”β”€β”€ configs/
β”‚   β”‚   β””β”€β”€ v100_2gpu_config.py       β† V100 μµμ ν™” μ„¤μ •
β”‚   β”β”€β”€ dataset.py
β”‚   β”β”€β”€ lora_config.py
β”‚   β”β”€β”€ train_lora.py
β”‚   β””β”€β”€ preprocess_resize.py
β”‚
β”β”€β”€ services/                          β† μ›Ή λ€μ‹λ³΄λ“
β”‚   β”β”€β”€ backend/ (FastAPI, 7010)
β”‚   β””β”€β”€ frontend/ (Next.js, 7020)
β”‚
β”β”€β”€ train_v100.sh                      β† μ „μ²΄ ν•™μµ μ¤ν¬λ¦½νΈ
β”β”€β”€ train_v100_test.sh                 β† μ¤λ²„ν”Όν… ν…μ¤νΈ
β”β”€β”€ create_test_dataset.py
β”β”€β”€ validate_data_quality.py
β”β”€β”€ inspect_samples.py
β”‚
β””β”€β”€ λ¬Έμ„/
    β”β”€β”€ V100_SETUP_GUIDE.md            β† V100 μƒμ„Έ κ°€μ΄λ“
    β”β”€β”€ V100_NEXT_STEPS.md             β† λ‹¤μ λ‹¨κ³„
    β”β”€β”€ DATA_QUALITY_GUIDE.md
    β”β”€β”€ PREPROCESSING_REPORT.md
    β””β”€β”€ services/README.md
```

## π€ λ°”λ΅ μ‹μ‘ν•κΈ°

### Step 1: μ²΄ν¬ν¬μΈνΈ λ‹¤μ΄λ΅λ“ (ν•„μ)

```bash
huggingface-cli download Wan-AI/Wan2.2-T2V-A14B --local-dir ./Wan2.2-T2V-A14B
```

### Step 2: μ¤λ²„ν”Όν… ν…μ¤νΈ (30λ¶„)

```bash
# μ‘μ€ λ°μ΄ν„°λ΅ λ¨Όμ € ν…μ¤νΈ
bash train_v100_test.sh
```

### Step 3: μ „μ²΄ ν•™μµ (5-7μΌ)

```bash
# ν…μ¤νΈ μ„±κ³µ ν›„
bash train_v100.sh
```

## π’΅ V100 μµμ ν™” ν•µμ‹¬

### λ©”λ¨λ¦¬ μ μ•½ μ „λµ

| ν•­λ© | μ„¤μ • | λ©”λ¨λ¦¬ μ μ•½ |
|------|------|------------|
| LoRA Rank | 16 (32β†’16) | 50% β¬‡οΈ |
| Frame Num | 49 (81β†’49) | 40% β¬‡οΈ |
| T5 CPU Offload | Yes | 10GB β¬‡οΈ |
| VAE CPU Offload | Yes | 5GB β¬‡οΈ |
| 8bit Optimizer | Yes | 30% β¬‡οΈ |

### Effective Batch Size μ μ§€

```
Effective Batch = batch_size Γ— gradient_accum Γ— num_gpus
                = 1 Γ— 16 Γ— 2 = 32 β…
```

## π“ μμƒ κ²°κ³Ό

### μ¤λ²„ν”Όν… ν…μ¤νΈ (100 μƒν”)
- β±οΈ **30-60λ¶„**
- π’Ύ **~30GB/GPU**
- π“‰ **Loss: 0.5 β†’ 0.01** (κ³Όμ ν•© μ„±κ³µ)

### μ „μ²΄ ν•™μµ (179,994 μƒν”)
- β±οΈ **5-7μΌ** (3 epochs)
- π’Ύ **~30GB/GPU**
- π“‰ **Loss: 0.15 β†’ 0.03**

## π― λ‹¤μ μ•΅μ…

### μ¦‰μ‹ μ‹¤ν–‰ κ°€λ¥ β…

1. **ν™κ²½ ν™•μΈ**
   ```bash
   nvidia-smi
   pip install bitsandbytes
   ```

2. **μ²΄ν¬ν¬μΈνΈ λ‹¤μ΄λ΅λ“**
   ```bash
   huggingface-cli download Wan-AI/Wan2.2-T2V-A14B --local-dir ./Wan2.2-T2V-A14B
   ```

3. **μ¤λ²„ν”Όν… ν…μ¤νΈ**
   ```bash
   bash train_v100_test.sh
   ```

### μ„ νƒ μ‚¬ν•­ (μ¶”μ²)

4. **λ°μ΄ν„° ν’μ§ κ²€μ¦**
   ```bash
   python validate_data_quality.py --sample_size 10
   python inspect_samples.py --num_samples 50
   ```

5. **μ›Ή λ€μ‹λ³΄λ“ μ‹¤ν–‰**
   ```bash
   cd services/backend && bash run.sh    # ν¬νΈ 7010
   cd services/frontend && npm run dev   # ν¬νΈ 7020
   ```

## π“ μ°Έκ³  λ¬Έμ„

1. **`V100_NEXT_STEPS.md`** - λ‹¤μ λ‹¨κ³„ μƒμ„Έ κ°€μ΄λ“
2. **`V100_SETUP_GUIDE.md`** - V100 μ„¤μ • λ° λ¬Έμ  ν•΄κ²°
3. **`DATA_QUALITY_GUIDE.md`** - λ°μ΄ν„° ν’μ§ κ²€μ¦
4. **`services/README.md`** - μ›Ή λ€μ‹λ³΄λ“

## β οΈ μ£Όμμ‚¬ν•­

### λ°λ“μ‹ ν™•μΈ
- β… GPU 2μ¥ μΈμ‹ (`nvidia-smi`)
- β… bitsandbytes μ„¤μΉ
- β… μ²΄ν¬ν¬μΈνΈ λ‹¤μ΄λ΅λ“ (~40GB)
- β… λ””μ¤ν¬ κ³µκ°„ (μµμ† 100GB)

### μ¤λ²„ν”Όν… ν…μ¤νΈ ν•„μ!
μ „μ²΄ ν•™μµ μ „ λ°λ“μ‹ μ‘μ€ λ°μ΄ν„°λ΅ ν…μ¤νΈν•μ—¬:
- Lossκ°€ κ°μ†ν•λ”μ§€ ν™•μΈ
- GPU λ©”λ¨λ¦¬κ°€ μ¶©λ¶„ν•μ§€ ν™•μΈ
- μ²΄ν¬ν¬μΈνΈ μ €μ¥λλ”μ§€ ν™•μΈ

## π‰ μ¤€λΉ„ μ™„λ£!

**λ¨λ“  μΈν”„λΌκ°€ μ¤€λΉ„λμ—μµλ‹λ‹¤.**
**V100 32GB Γ— 2λ΅ Wan2.2 LoRA fine-tuningμ„ μ‹μ‘ν•  μ μμµλ‹λ‹¤!**

```bash
# μ§€κΈ λ°”λ΅ μ‹μ‘!
bash train_v100_test.sh
```

**Good Luck! π€**
