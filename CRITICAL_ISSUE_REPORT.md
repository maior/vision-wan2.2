# Critical Issue Report - LoRA Training

**Date**: 2025-11-10 21:54
**Status**: ğŸ”´ **CRITICAL - Training Blocked**

## Problem Summary

LoRA í•™ìŠµì´ ì²« ë²ˆì§¸ stepì„ ì™„ë£Œí•˜ì§€ ëª»í•˜ê³  ìˆìŠµë‹ˆë‹¤.

### ì£¼ìš” ì¦ìƒ

1. âŒ **VAE Encoding Hang**
   - Forward pass í…ŒìŠ¤íŠ¸ì—ì„œ VAE encoding ë‹¨ê³„ì—ì„œ 5ë¶„ ì´ìƒ ì‘ë‹µ ì—†ìŒ
   - GPU í™œìš©ë¥  0% ìƒíƒœì—ì„œ í”„ë¡œì„¸ìŠ¤ë§Œ ë©”ëª¨ë¦¬ ì ìœ 

2. âŒ **JSON ë¡œê·¸ ë¯¸ìƒì„±**
   - `training_logs/training_gpu0.json` íŒŒì¼ ìƒì„±ë˜ì§€ ì•ŠìŒ
   - ì²« step ì™„ë£Œë˜ì§€ ì•Šì•„ ë¡œê¹…ì´ ì‹œì‘ë˜ì§€ ì•ŠìŒ

3. âŒ **í”„ë¡ íŠ¸ì—”ë“œ í‘œì‹œ ë¶ˆê°€**
   - ë°±ì—”ë“œ APIëŠ” ì •ìƒ ì‘ë™ (`/api/training/status`, `/api/training/metrics`)
   - JSON íŒŒì¼ì´ ì—†ì–´ ë¹ˆ ë°ì´í„° ë°˜í™˜ â†’ í”„ë¡ íŠ¸ì—”ë“œì—ì„œ "inactive" í‘œì‹œ

4. âŒ **í”„ë¡œì„¸ìŠ¤ ì¤‘ë³µ ìƒì„±**
   - ë°±ê·¸ë¼ìš´ë“œ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì‹œ í”„ë¡œì„¸ìŠ¤ê°€ ê³„ì† ì¦ê°€ (ìµœëŒ€ 6ê°œ í™•ì¸)
   - ë©”ëª¨ë¦¬ ê°„ì„­ ë° ë¦¬ì†ŒìŠ¤ ë‚­ë¹„

## Technical Details

### VAE Encoding Bottleneck

```
File: /home/maiordba/projects/vision/Wan2.2/lora_finetuning/train_lora.py
Lines: 172-180

# VAE encodingì—ì„œ hang ë°œìƒ
with torch.no_grad():
    video_list = [videos[i] for i in range(B)]
    latents_list = model.vae.encode(video_list)  # â¬…ï¸ ì—¬ê¸°ì„œ ë©ˆì¶¤
    latents = torch.stack([l.to(rank) for l in latents_list])
```

**í…ŒìŠ¤íŠ¸ ê²°ê³¼:**
- Test forward pass with batch_size=1, resolution=1280x704, frames=17
- Timeout after 300 seconds (5 minutes)
- Process stuck at VAE autocast line

### Current Implementation

```python
# lora_finetuning/train_lora.py:168-228
# ì‹¤ì œ êµ¬í˜„ ì™„ë£Œ:
1. VAE encoding (frozen) âœ… ì½”ë“œ ì‘ì„± ì™„ë£Œ, ì‹¤í–‰ ì‹œ hang âŒ
2. Text encoding (frozen T5 on CPU) âœ…
3. Diffusion noise addition âœ…
4. DiT forward with LoRA âœ…
5. MSE loss calculation âœ…
```

## Root Causes

### 1. VAE Performance Issue
- Wan2_2_VAEëŠ” 4Ã—16Ã—16 ì••ì¶•ë¥ ì„ ì‚¬ìš©
- 1280x704x17 ë¹„ë””ì˜¤ â†’ ë§¤ìš° í° ì—°ì‚°ëŸ‰
- Batch processingì—ì„œ ë©”ëª¨ë¦¬ ë˜ëŠ” ì—°ì‚° ë³‘ëª© ë°œìƒ

### 2. Process Management
- ë°±ê·¸ë¼ìš´ë“œ ìŠ¤í¬ë¦½íŠ¸(`train_quick_test.sh`) ì‹¤í–‰ ì‹œ ì¤‘ë³µ í”„ë¡œì„¸ìŠ¤ ìƒì„±
- `nohup`, `&`, `bash` ì¡°í•©ì˜ ë¬¸ì œ

### 3. Data Loading
- DataLoaderì˜ `num_workers=4` ì„¤ì •
- ì²« batch ë¡œë”© ì‹œê°„ì´ ê¸¸ ìˆ˜ ìˆìŒ

## Attempted Solutions

### âœ… Completed
1. T5 CPU-GPU ë°ì´í„° ì´ë™ ìˆ˜ì •
2. GPU ì‹ë³„ ë¡œì§ ìˆ˜ì • (CUDA_VISIBLE_DEVICES ê¸°ë°˜)
3. ì‹¤ì œ forward pass êµ¬í˜„ (VAE, Text, DiT, Loss)
4. ë°±ì—”ë“œ API ì •ìƒ í™•ì¸

### âŒ Failed
1. Forward pass í…ŒìŠ¤íŠ¸ - VAE encodingì—ì„œ íƒ€ì„ì•„ì›ƒ
2. í•™ìŠµ ì¬ì‹œì‘ ì‹œë„ - ë™ì¼í•œ hang ë°œìƒ
3. í”„ë¡œì„¸ìŠ¤ ê´€ë¦¬ - ì¤‘ë³µ ìƒì„± ë¬¸ì œ ì§€ì†

## Proposed Solutions

### Option 1: VAEë¥¼ CPUë¡œ ì´ë™ (ê¶Œì¥í•˜ì§€ ì•ŠìŒ)
- VAEë¥¼ CPUì—ì„œ ì‹¤í–‰í•˜ë©´ ë§¤ìš° ëŠë¦¼
- GPU ë©”ëª¨ë¦¬ëŠ” ì¶©ë¶„ (32GB ì¤‘ 23GB ì‚¬ìš©)

### Option 2: Batch Size ê°ì†Œ
- í˜„ì¬: batch_size=2
- ë³€ê²½: batch_size=1
- ë‹¨ì : í•™ìŠµ ì†ë„ ê°ì†Œ

### Option 3: í•´ìƒë„ ê°ì†Œ
- í˜„ì¬: 1280x704 (TI2V-5B í‘œì¤€)
- ë³€ê²½: 704x480 ë“±
- ë‹¨ì : ëª¨ë¸ì˜ ì›ë˜ í•™ìŠµ í•´ìƒë„ì™€ ë¶ˆì¼ì¹˜

### Option 4: Dummy Lossë¡œ ìš°ì„  í…ŒìŠ¤íŠ¸ â­ **ì¶”ì²œ**
- JSON ë¡œê·¸ ìƒì„± ë¡œì§ë§Œ ë¨¼ì € í…ŒìŠ¤íŠ¸
- í”„ë¡ íŠ¸ì—”ë“œ í‘œì‹œ í™•ì¸
- VAE ë¬¸ì œ ë¶„ë¦¬ ë””ë²„ê¹…

### Option 5: DataLoader ìˆ˜ì •
- `num_workers` ê°ì†Œ (4 â†’ 0 ë˜ëŠ” 1)
- `pin_memory=False`
- ì²« batch ë¡œë”© ë¬¸ì œ ê°€ëŠ¥ì„±

## Current Status

```
í•™ìŠµ í”„ë¡œì„¸ìŠ¤: ì •ë¦¬ë¨
GPU ìƒíƒœ: ëŒ€ê¸° ì¤‘
ë°±ì—”ë“œ API: ì •ìƒ ì‘ë™
í”„ë¡ íŠ¸ì—”ë“œ: JSON ë¡œê·¸ ëŒ€ê¸° ì¤‘
```

## Next Steps

1. **ì¦‰ì‹œ**: Dummy lossë¡œ JSON ë¡œê·¸ ìƒì„± í…ŒìŠ¤íŠ¸
2. **ë””ë²„ê¹…**: VAE encoding ë³‘ëª© ì›ì¸ íŒŒì•…
3. **ì¥ê¸°**: ì‹¤ì œ í•™ìŠµ ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸

## Files Modified

```
/home/maiordba/projects/vision/Wan2.2/lora_finetuning/train_lora.py
  - Lines 172-228: Forward pass êµ¬í˜„
  - Lines 338-348: GPU identification ìˆ˜ì •
  - Lines 283-341: Validation forward pass

/home/maiordba/projects/vision/Wan2.2/test_forward.py
  - Forward pass í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ (íƒ€ì„ì•„ì›ƒ í™•ì¸)
```

## Contact

Claude Code - AI Assistant
Session: 2025-11-10
