#!/bin/bash
# 24ì‹œê°„ í…ŒìŠ¤íŠ¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (5,000 ìƒ˜í”Œ)

set -e

echo "========================================="
echo "Wan2.2 LoRA Quick Test (24h)"
echo "========================================="

# ê°€ìƒí™˜ê²½ í™œì„±í™”
if [ -d ".venv" ]; then
    echo "ê°€ìƒí™˜ê²½ í™œì„±í™”: .venv"
    source .venv/bin/activate
else
    echo "ERROR: ê°€ìƒí™˜ê²½ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    exit 1
fi

export OMP_NUM_THREADS=8

# ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬
CKPT_DIR="./Wan2.2-TI2V-5B"

echo ""
echo "ì„¤ì •:"
echo "  - í…ŒìŠ¤íŠ¸ ëª¨ë“œ: 5,000 train samples (2,500 per GPU)"
echo "  - Batch size: 2 per GPU"
echo "  - Expected time: 10-12 hours"
echo "  - Backend API: http://localhost:7010/api/training"
echo ""

# ì²´í¬í¬ì¸íŠ¸ í™•ì¸
if [ ! -d "$CKPT_DIR" ]; then
    echo "ERROR: ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: $CKPT_DIR"
    exit 1
fi

# ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p "./lora_checkpoints_quick_gpu0"
mkdir -p "./lora_checkpoints_quick_gpu1"
mkdir -p "./training_logs"

# ê¸°ì¡´ ë¡œê·¸ ì‚­ì œ
rm -f train_lora_gpu0.log train_lora_gpu1.log
rm -f ./training_logs/*.json

echo "GPU 0, GPU 1 ë³‘ë ¬ í•™ìŠµ ì‹œì‘..."
echo ""

# GPU 0 í•™ìŠµ (ë°±ê·¸ë¼ìš´ë“œ)
CUDA_VISIBLE_DEVICES=0 python lora_finetuning/train_lora.py \
    --train_csv "./data_quality_analysis/quick_train_gpu0.csv" \
    --val_csv "./data_quality_analysis_val/quick_val_gpu0.csv" \
    --ckpt_dir "$CKPT_DIR" \
    --output_dir "./lora_checkpoints_quick_gpu0" \
    > train_lora_gpu0.log 2>&1 &

GPU0_PID=$!
echo "âœ“ GPU 0 í•™ìŠµ ì‹œì‘ (PID: $GPU0_PID)"

# GPU 0 ëª¨ë¸ ë¡œë”© ëŒ€ê¸° (ì•½ 2.5ë¶„)
echo "â³ GPU 0 ëª¨ë¸ ë¡œë”© ëŒ€ê¸° ì¤‘... (150ì´ˆ)"
sleep 150

# GPU 1 í•™ìŠµ (ë°±ê·¸ë¼ìš´ë“œ)
CUDA_VISIBLE_DEVICES=1 python lora_finetuning/train_lora.py \
    --train_csv "./data_quality_analysis/quick_train_gpu1.csv" \
    --val_csv "./data_quality_analysis_val/quick_val_gpu1.csv" \
    --ckpt_dir "$CKPT_DIR" \
    --output_dir "./lora_checkpoints_quick_gpu1" \
    > train_lora_gpu1.log 2>&1 &

GPU1_PID=$!
echo "âœ“ GPU 1 í•™ìŠµ ì‹œì‘ (PID: $GPU1_PID)"

# Backend APIëŠ” ì´ë¯¸ services/backendì—ì„œ ì‹¤í–‰ ì¤‘
# ë³„ë„ ëŒ€ì‹œë³´ë“œ ì„œë²„ ë¶ˆí•„ìš”

echo ""
echo "========================================="
echo "24ì‹œê°„ í…ŒìŠ¤íŠ¸ í•™ìŠµ ì‹¤í–‰ ì¤‘!"
echo "  - GPU 0 PID: $GPU0_PID"
echo "  - GPU 1 PID: $GPU1_PID"
echo ""
echo "ğŸ“Š ëª¨ë‹ˆí„°ë§:"
echo "  - Backend API: http://localhost:7010/api/training/status"
echo "  - Frontend: http://localhost:7020"
echo ""
echo "ë¡œê·¸ í™•ì¸:"
echo "  tail -f train_lora_gpu0.log"
echo "  tail -f train_lora_gpu1.log"
echo "========================================="
echo ""

# í”„ë¡œì„¸ìŠ¤ ëŒ€ê¸°
wait $GPU0_PID
GPU0_EXIT=$?
wait $GPU1_PID
GPU1_EXIT=$?

echo ""
echo "========================================="
echo "í•™ìŠµ ì™„ë£Œ!"
echo "  - GPU 0 exit code: $GPU0_EXIT"
echo "  - GPU 1 exit code: $GPU1_EXIT"
echo ""
echo "ì²´í¬í¬ì¸íŠ¸:"
echo "  - GPU 0: ./lora_checkpoints_quick_gpu0"
echo "  - GPU 1: ./lora_checkpoints_quick_gpu1"
echo "========================================="

if [ $GPU0_EXIT -eq 0 ] && [ $GPU1_EXIT -eq 0 ]; then
    exit 0
else
    exit 1
fi
