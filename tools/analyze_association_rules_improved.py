#!/usr/bin/env python3
"""
ê°œì„ ëœ ì—°ê´€ê·œì¹™ ë¶„ì„ - ëª…ì‚¬ ê¸°ë°˜ + í´ëŸ¬ìŠ¤í„° ëŒ€í‘œ í‚¤ì›Œë“œ
"""

import pandas as pd
import numpy as np
from pathlib import Path
import json
from collections import Counter, defaultdict
from datetime import datetime

# Korean NLP
from kiwipiepy import Kiwi

# Association rules
from mlxtend.frequent_patterns import apriori, association_rules
from mlxtend.preprocessing import TransactionEncoder

# Clustering
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import MiniBatchKMeans

# ê²½ë¡œ ì„¤ì •
INPUT_CSV = '/home/maiordba/projects/vision/Wan2.2/preprocessed_data/all_train.csv'
OUTPUT_DIR = Path('/home/maiordba/projects/vision/Wan2.2/data_quality_analysis')
OUTPUT_DIR.mkdir(exist_ok=True)

RULES_JSON = OUTPUT_DIR / 'association_rules.json'
PATTERNS_JSON = OUTPUT_DIR / 'keyword_patterns.json'
SAMPLING_JSON = OUTPUT_DIR / 'advanced_sampling_results.json'

SAMPLE_SIZE_FOR_RULES = 50000

print("=" * 80)
print("ê°œì„ ëœ ì—°ê´€ê·œì¹™ ë¶„ì„ (ëª…ì‚¬ ê¸°ë°˜ + í´ëŸ¬ìŠ¤í„° ì„¤ëª…)")
print("=" * 80)

# 1. Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”
print(f"\nðŸ”§ Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì¤‘...")
kiwi = Kiwi()
print(f"âœ“ ì´ˆê¸°í™” ì™„ë£Œ")

# 2. ë°ì´í„° ë¡œë“œ
print(f"\nðŸ“‚ ë°ì´í„° ë¡œë”© ì¤‘...")
df = pd.read_csv(INPUT_CSV, usecols=['clip_id', 'file_path', 'caption'])
print(f"âœ“ ì´ {len(df):,}ê°œ ìƒ˜í”Œ ë¡œë“œ")

# 3. ëª…ì‚¬ ì¶”ì¶œ í•¨ìˆ˜
def extract_nouns(text, min_length=2, max_nouns=20):
    """Kiwië¥¼ ì‚¬ìš©í•˜ì—¬ ëª…ì‚¬ë§Œ ì¶”ì¶œ"""
    if pd.isna(text):
        return []
    
    try:
        result = kiwi.analyze(str(text))
        if not result:
            return []
        
        # ëª…ì‚¬(NNG, NNP) ì¶”ì¶œ
        nouns = []
        for token, pos, _, _ in result[0][0]:
            if pos in ('NNG', 'NNP') and len(token) >= min_length:
                # ìˆ«ìžë¡œë§Œ ì´ë£¨ì–´ì§„ ë‹¨ì–´ ì œì™¸
                if not token.isdigit():
                    nouns.append(token)
        
        # ì¤‘ë³µ ì œê±° ë° ìƒìœ„ Nê°œë§Œ
        return list(set(nouns))[:max_nouns]
    except:
        return []

# 4. ì—°ê´€ê·œì¹™ ë¶„ì„ìš© ìƒ˜í”Œë§
print(f"\nðŸŽ² ì—°ê´€ê·œì¹™ ë¶„ì„ìš© {SAMPLE_SIZE_FOR_RULES:,}ê°œ ìƒ˜í”Œë§...")
if len(df) > SAMPLE_SIZE_FOR_RULES:
    rules_df = df.sample(n=SAMPLE_SIZE_FOR_RULES, random_state=42)
else:
    rules_df = df.copy()

print(f"âœ“ ìƒ˜í”Œë§ ì™„ë£Œ: {len(rules_df):,}ê°œ")

# 5. ëª…ì‚¬ ì¶”ì¶œ (ë°°ì¹˜ ì²˜ë¦¬)
print(f"\nðŸ” ëª…ì‚¬ ì¶”ì¶œ ì¤‘ (Kiwi ì‚¬ìš©)...")
nouns_list = []
for i, caption in enumerate(rules_df['caption']):
    nouns = extract_nouns(caption)
    nouns_list.append(nouns)
    
    if (i + 1) % 5000 == 0:
        print(f"  ì§„í–‰: {i+1:,}/{len(rules_df):,}")

rules_df['nouns'] = nouns_list

# 6. ì „ì²´ ëª…ì‚¬ ë¹ˆë„ ê³„ì‚°
all_nouns = []
for nouns in rules_df['nouns']:
    all_nouns.extend(nouns)

noun_freq = Counter(all_nouns)

# ìƒìœ„ 50ê°œ ëª…ì‚¬ë§Œ ì‚¬ìš©
top_nouns = set([n for n, _ in noun_freq.most_common(50)])

print(f"âœ“ ì „ì²´ ê³ ìœ  ëª…ì‚¬: {len(noun_freq):,}ê°œ")
print(f"âœ“ ë¶„ì„ ëŒ€ìƒ ëª…ì‚¬: {len(top_nouns)}ê°œ")

# ìƒìœ„ 30ê°œ ëª…ì‚¬ ì¶œë ¥
print(f"\nðŸ“Š ê°€ìž¥ ë¹ˆë²ˆí•œ ëª…ì‚¬ Top 30:")
for i, (noun, count) in enumerate(noun_freq.most_common(30), 1):
    pct = (count / len(rules_df)) * 100
    print(f"  {i:2d}. {noun:15} {count:6,}ê°œ ({pct:5.1f}%)")

# 7. íŠ¸ëžœìž­ì…˜ ìƒì„±
print(f"\nðŸ“¦ íŠ¸ëžœìž­ì…˜ ìƒì„± ì¤‘...")
transactions = []
for nouns in rules_df['nouns']:
    transaction = [n for n in nouns if n in top_nouns]
    if transaction:
        transactions.append(list(set(transaction)))

print(f"âœ“ ìƒì„±ëœ íŠ¸ëžœìž­ì…˜: {len(transactions):,}ê°œ")

# 8. Apriori ì•Œê³ ë¦¬ì¦˜ ì‹¤í–‰
print(f"\nâš™ï¸  Apriori ì•Œê³ ë¦¬ì¦˜ ì‹¤í–‰ ì¤‘...")

te = TransactionEncoder()
te_ary = te.fit(transactions).transform(transactions)
trans_df = pd.DataFrame(te_ary, columns=te.columns_)

print(f"âœ“ íŠ¸ëžœìž­ì…˜ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±: {trans_df.shape}")

min_support = 0.02
frequent_itemsets = apriori(trans_df, min_support=min_support, use_colnames=True, max_len=3)

print(f"âœ“ ë°œê²¬ëœ ë¹ˆë²ˆí•œ íŒ¨í„´: {len(frequent_itemsets)}ê°œ")

# 9. ì—°ê´€ê·œì¹™ ìƒì„±
rules_data = []
if len(frequent_itemsets) > 0:
    print(f"\nðŸ”— ì—°ê´€ê·œì¹™ ìƒì„± ì¤‘...")
    
    rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=0.4, num_itemsets=len(frequent_itemsets))
    
    if len(rules) > 0:
        rules = rules.sort_values('lift', ascending=False)
        print(f"âœ“ ë°œê²¬ëœ ì—°ê´€ê·œì¹™: {len(rules)}ê°œ")
        
        print(f"\nðŸ† ê°•ë ¥í•œ ì—°ê´€ê·œì¹™ Top 15 (Lift ê¸°ì¤€):")
        for idx, row in rules.head(15).iterrows():
            antecedents = ', '.join(list(row['antecedents']))
            consequents = ', '.join(list(row['consequents']))
            print(f"  {antecedents:20} => {consequents:20}")
            print(f"    Sup: {row['support']:.3f} | Conf: {row['confidence']:.3f} | Lift: {row['lift']:.2f}")
        
        for idx, row in rules.iterrows():
            rules_data.append({
                'antecedents': list(row['antecedents']),
                'consequents': list(row['consequents']),
                'support': float(row['support']),
                'confidence': float(row['confidence']),
                'lift': float(row['lift'])
            })
        
        with open(RULES_JSON, 'w', encoding='utf-8') as f:
            json.dump({
                'timestamp': datetime.now().isoformat(),
                'total_transactions': len(transactions),
                'total_rules': len(rules),
                'rules': rules_data[:50]
            }, f, indent=2, ensure_ascii=False)
        
        print(f"âœ“ ì—°ê´€ê·œì¹™ ì €ìž¥: {RULES_JSON}")

# 10. í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬
print(f"\nðŸ“ˆ í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ìƒì„± ì¤‘...")

keyword_cooccurrence = defaultdict(lambda: defaultdict(int))

for transaction in transactions:
    for i, kw1 in enumerate(transaction):
        for kw2 in transaction[i+1:]:
            keyword_cooccurrence[kw1][kw2] += 1
            keyword_cooccurrence[kw2][kw1] += 1

edges = []
for kw1, connections in keyword_cooccurrence.items():
    for kw2, count in connections.items():
        if count > len(transactions) * 0.01:
            edges.append({
                'source': kw1,
                'target': kw2,
                'weight': count,
                'support': count / len(transactions)
            })

edges = sorted(edges, key=lambda x: x['weight'], reverse=True)[:30]

nodes = [{'id': n, 'count': c, 'frequency': c / len(rules_df)} 
         for n, c in noun_freq.most_common(30)]

with open(PATTERNS_JSON, 'w', encoding='utf-8') as f:
    json.dump({
        'timestamp': datetime.now().isoformat(),
        'nodes': nodes,
        'edges': edges
    }, f, indent=2, ensure_ascii=False)

print(f"âœ“ í‚¤ì›Œë“œ íŒ¨í„´ ì €ìž¥: {PATTERNS_JSON}")

# 11. í´ëŸ¬ìŠ¤í„°ë§ (ì „ì²´ ë°ì´í„°)
print(f"\nðŸŽ¯ í´ëŸ¬ìŠ¤í„°ë§ (ì „ì²´ ë°ì´í„°ì…‹)...")

print(f"  1) TF-IDF ë²¡í„°í™”...")
vectorizer = TfidfVectorizer(max_features=100, min_df=10, max_df=0.5)
tfidf_matrix = vectorizer.fit_transform(df['caption'].fillna(''))

print(f"  2) MiniBatch K-Means í´ëŸ¬ìŠ¤í„°ë§ (K=15)...")
n_clusters = 15
kmeans = MiniBatchKMeans(n_clusters=n_clusters, random_state=42, batch_size=1000, n_init=3)
df['cluster'] = kmeans.fit_predict(tfidf_matrix)

print(f"âœ“ í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ")

# 12. ê° í´ëŸ¬ìŠ¤í„°ì˜ ëŒ€í‘œ ëª…ì‚¬ ì¶”ì¶œ
print(f"\n  3) í´ëŸ¬ìŠ¤í„°ë³„ ëŒ€í‘œ í‚¤ì›Œë“œ ì¶”ì¶œ...")

# ì „ì²´ ë°ì´í„°ì—ì„œ ëª…ì‚¬ ì¶”ì¶œ (ìƒ˜í”Œë§)
print(f"     ì „ì²´ ë°ì´í„°ì—ì„œ ëª…ì‚¬ ì¶”ì¶œ ì¤‘ (10K ìƒ˜í”Œ)...")
sample_for_nouns = df.sample(n=min(10000, len(df)), random_state=42)
sample_nouns = []
for i, caption in enumerate(sample_for_nouns['caption']):
    nouns = extract_nouns(caption, min_length=2)
    sample_nouns.append(nouns)
    if (i + 1) % 2000 == 0:
        print(f"       {i+1:,}/10,000")

sample_for_nouns['nouns'] = sample_nouns

# í´ëŸ¬ìŠ¤í„°ë³„ ëŒ€í‘œ ëª…ì‚¬
cluster_info = []
for cluster_id in range(n_clusters):
    cluster_samples = sample_for_nouns[sample_for_nouns['cluster'] == cluster_id]
    total_in_cluster = len(df[df['cluster'] == cluster_id])
    
    # í´ëŸ¬ìŠ¤í„° ë‚´ ëª…ì‚¬ ë¹ˆë„
    cluster_nouns = []
    for nouns in cluster_samples['nouns']:
        cluster_nouns.extend(nouns)
    
    top_cluster_nouns = Counter(cluster_nouns).most_common(5)
    
    cluster_info.append({
        'cluster_id': cluster_id,
        'size': total_in_cluster,
        'percentage': total_in_cluster / len(df) * 100,
        'top_keywords': [{'keyword': n, 'count': c} for n, c in top_cluster_nouns],
        'description': ', '.join([n for n, _ in top_cluster_nouns[:3]])  # ìƒìœ„ 3ê°œë¡œ ì„¤ëª…
    })

print(f"âœ“ í´ëŸ¬ìŠ¤í„° ëŒ€í‘œ í‚¤ì›Œë“œ ìƒì„± ì™„ë£Œ")

# 13. í´ëŸ¬ìŠ¤í„° ë¶„í¬ ì¶œë ¥
print(f"\nðŸ“Š í´ëŸ¬ìŠ¤í„° ë¶„í¬ (ëŒ€í‘œ í‚¤ì›Œë“œ í¬í•¨):")
for info in sorted(cluster_info, key=lambda x: x['size'], reverse=True):
    print(f"  í´ëŸ¬ìŠ¤í„° {info['cluster_id']:2d}: {info['size']:6,}ê°œ ({info['percentage']:5.1f}%)")
    print(f"    ëŒ€í‘œ í‚¤ì›Œë“œ: {info['description']}")

# 14. ë‹¤ì–‘ì„± ê¸°ë°˜ ìƒ˜í”Œë§
print(f"\n  4) ë‹¤ì–‘ì„± ê¸°ë°˜ ìƒ˜í”Œë§ (50K)...")

TARGET_SAMPLES = 50000
samples_per_cluster = TARGET_SAMPLES // n_clusters

selected_indices = []

for cluster_id in range(n_clusters):
    cluster_df = df[df['cluster'] == cluster_id]
    n_select = min(samples_per_cluster, len(cluster_df))
    
    if n_select > 0:
        sampled = cluster_df.sample(n=n_select, random_state=42)
        selected_indices.extend(sampled.index.tolist())

if len(selected_indices) < TARGET_SAMPLES:
    remaining = TARGET_SAMPLES - len(selected_indices)
    remaining_indices = list(set(range(len(df))) - set(selected_indices))
    additional = np.random.choice(remaining_indices, size=min(remaining, len(remaining_indices)), replace=False)
    selected_indices.extend(additional)

print(f"âœ“ ì„ íƒëœ ìƒ˜í”Œ: {len(selected_indices):,}ê°œ")

# 15. ê²°ê³¼ ì €ìž¥
selected_df = df.loc[selected_indices]

output_df = pd.DataFrame({
    'video': selected_df['file_path'],
    'prompt': selected_df['caption']
})

advanced_csv = OUTPUT_DIR / 'train_metadata_50k_advanced.csv'
output_df.to_csv(advanced_csv, index=False)

selected_cluster_dist = selected_df['cluster'].value_counts().sort_index()

sampling_results = {
    'timestamp': datetime.now().isoformat(),
    'method': 'diversity_based_clustering',
    'total_samples': len(selected_df),
    'n_clusters': n_clusters,
    'cluster_distribution': [
        {
            'cluster_id': int(cid),
            'count': int(count),
            'percentage': float(count / len(selected_df) * 100),
            'description': cluster_info[cid]['description'],
            'top_keywords': cluster_info[cid]['top_keywords']
        }
        for cid, count in selected_cluster_dist.items()
    ]
}

with open(SAMPLING_JSON, 'w', encoding='utf-8') as f:
    json.dump(sampling_results, f, indent=2, ensure_ascii=False)

print(f"âœ“ ê³ ê¸‰ ìƒ˜í”Œë§ CSV: {advanced_csv}")
print(f"âœ“ ìƒ˜í”Œë§ ê²°ê³¼ JSON: {SAMPLING_JSON}")

print(f"\nâœ… ë¶„ì„ ì™„ë£Œ!")
print(f"\nðŸ“‹ ìš”ì•½:")
print(f"  - ì—°ê´€ê·œì¹™: {len(rules_data)}ê°œ (ëª…ì‚¬ ê¸°ë°˜)")
print(f"  - í‚¤ì›Œë“œ ì—£ì§€: {len(edges)}ê°œ")
print(f"  - í´ëŸ¬ìŠ¤í„°: {n_clusters}ê°œ (ëŒ€í‘œ í‚¤ì›Œë“œ í¬í•¨)")
print(f"  - ì„ íƒëœ ìƒ˜í”Œ: {len(selected_df):,}ê°œ")
