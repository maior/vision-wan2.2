#!/usr/bin/env python3
"""
ì—°ê´€ê·œì¹™ ë¶„ì„ì„ í†µí•œ ë°ì´í„°ì…‹ íŒ¨í„´ ë°œê²¬ ë° ê³ ê¸‰ ìƒ˜í”Œë§
ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë²„ì „
"""

import pandas as pd
import numpy as np
from pathlib import Path
import json
import re
from collections import Counter, defaultdict
from datetime import datetime
import sys

# Association rules
from mlxtend.frequent_patterns import apriori, association_rules
from mlxtend.preprocessing import TransactionEncoder

# Clustering for diversity
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import MiniBatchKMeans
from sklearn.metrics.pairwise import cosine_distances

# ê²½ë¡œ ì„¤ì •
INPUT_CSV = '/home/maiordba/projects/vision/Wan2.2/preprocessed_data/all_train.csv'
OUTPUT_DIR = Path('/home/maiordba/projects/vision/Wan2.2/data_quality_analysis')
OUTPUT_DIR.mkdir(exist_ok=True)

# ë¶„ì„ ê²°ê³¼ ì €ì¥ ê²½ë¡œ
RULES_JSON = OUTPUT_DIR / 'association_rules.json'
PATTERNS_JSON = OUTPUT_DIR / 'keyword_patterns.json'
SAMPLING_JSON = OUTPUT_DIR / 'advanced_sampling_results.json'

# ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•œ ìƒ˜í”Œë§
SAMPLE_SIZE_FOR_RULES = 50000  # ì—°ê´€ê·œì¹™ ë¶„ì„ìš© ìƒ˜í”Œ í¬ê¸°

print("=" * 80)
print("ì—°ê´€ê·œì¹™ ê¸°ë°˜ ë°ì´í„°ì…‹ ë¶„ì„ ë° ê³ ê¸‰ ìƒ˜í”Œë§ (ë©”ëª¨ë¦¬ íš¨ìœ¨í™” ë²„ì „)")
print("=" * 80)

# 1. ë°ì´í„° ë¡œë“œ (í•„ìš”í•œ ì»¬ëŸ¼ë§Œ)
print(f"\nğŸ“‚ ë°ì´í„° ë¡œë”© ì¤‘...")
df = pd.read_csv(INPUT_CSV, usecols=['clip_id', 'file_path', 'caption'])
print(f"âœ“ ì´ {len(df):,}ê°œ ìƒ˜í”Œ ë¡œë“œ")

# 2. í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜
def extract_keywords(text, min_length=2, max_keywords=20):
    """í…ìŠ¤íŠ¸ì—ì„œ ì˜ë¯¸ìˆëŠ” í‚¤ì›Œë“œ ì¶”ì¶œ (ë©”ëª¨ë¦¬ íš¨ìœ¨í™”)"""
    if pd.isna(text):
        return []
    
    # í•œê¸€ë§Œ ì¶”ì¶œ (ì˜ë¬¸ ì œì™¸ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½)
    words = re.findall(r'[ê°€-í£]{2,}', str(text))
    
    # ë¶ˆìš©ì–´ ì œê±°
    stopwords = {'ìˆëŠ”', 'ìˆë‹¤', 'ë˜ëŠ”', 'ë˜ë‹¤', 'í•˜ëŠ”', 'í•˜ë‹¤', 'ì´ë‹¤', 'ê·¸', 'ì €', 'ê²ƒ', 
                 'ìˆìŠµë‹ˆë‹¤', 'ë©ë‹ˆë‹¤', 'í•©ë‹ˆë‹¤', 'ì—ì„œ', 'ì—ê²Œ', 'ìœ¼ë¡œ', 'ë¥¼', 'ì„', 'ê°€', 'ì´'}
    keywords = [w for w in words if w not in stopwords]
    
    # ìƒìœ„ Nê°œë§Œ ë°˜í™˜
    return list(set(keywords))[:max_keywords]

# 3. ì—°ê´€ê·œì¹™ ë¶„ì„ìš© ìƒ˜í”Œë§
print(f"\nğŸ² ì—°ê´€ê·œì¹™ ë¶„ì„ìš© {SAMPLE_SIZE_FOR_RULES:,}ê°œ ìƒ˜í”Œë§...")
if len(df) > SAMPLE_SIZE_FOR_RULES:
    rules_df = df.sample(n=SAMPLE_SIZE_FOR_RULES, random_state=42)
else:
    rules_df = df.copy()

print(f"âœ“ ìƒ˜í”Œë§ ì™„ë£Œ: {len(rules_df):,}ê°œ")

# 4. í‚¤ì›Œë“œ ì¶”ì¶œ
print(f"\nğŸ” í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
rules_df['keywords'] = rules_df['caption'].apply(extract_keywords)

# ì „ì²´ í‚¤ì›Œë“œ ë¹ˆë„ ê³„ì‚°
all_keywords = []
for i, kws in enumerate(rules_df['keywords']):
    all_keywords.extend(kws)
    if (i + 1) % 10000 == 0:
        print(f"  ì§„í–‰: {i+1:,}/{len(rules_df):,}")

keyword_freq = Counter(all_keywords)

# ìƒìœ„ 50ê°œ í‚¤ì›Œë“œë§Œ ì‚¬ìš©
top_keywords = set([k for k, _ in keyword_freq.most_common(50)])

print(f"âœ“ ì „ì²´ ê³ ìœ  í‚¤ì›Œë“œ: {len(keyword_freq):,}ê°œ")
print(f"âœ“ ë¶„ì„ ëŒ€ìƒ í‚¤ì›Œë“œ: {len(top_keywords)}ê°œ")

# ìƒìœ„ 20ê°œ í‚¤ì›Œë“œ ì¶œë ¥
print(f"\nğŸ“Š ê°€ì¥ ë¹ˆë²ˆí•œ í‚¤ì›Œë“œ Top 20:")
for i, (keyword, count) in enumerate(keyword_freq.most_common(20), 1):
    pct = (count / len(rules_df)) * 100
    print(f"  {i:2d}. {keyword:15} {count:6,}ê°œ ({pct:5.1f}%)")

# 5. íŠ¸ëœì­ì…˜ ìƒì„±
print(f"\nğŸ“¦ íŠ¸ëœì­ì…˜ ìƒì„± ì¤‘...")
transactions = []
for kws in rules_df['keywords']:
    transaction = [k for k in kws if k in top_keywords]
    if transaction:
        transactions.append(list(set(transaction)))

print(f"âœ“ ìƒì„±ëœ íŠ¸ëœì­ì…˜: {len(transactions):,}ê°œ")

# 6. Apriori ì•Œê³ ë¦¬ì¦˜ ì‹¤í–‰
print(f"\nâš™ï¸  Apriori ì•Œê³ ë¦¬ì¦˜ ì‹¤í–‰ ì¤‘...")

te = TransactionEncoder()
te_ary = te.fit(transactions).transform(transactions)
trans_df = pd.DataFrame(te_ary, columns=te.columns_)

print(f"âœ“ íŠ¸ëœì­ì…˜ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±: {trans_df.shape}")

# Apriorië¡œ ë¹ˆë²ˆí•œ ì•„ì´í…œì…‹ ì°¾ê¸°
min_support = 0.02  # 2% ì§€ì§€ë„
frequent_itemsets = apriori(trans_df, min_support=min_support, use_colnames=True, max_len=3)

print(f"âœ“ ë°œê²¬ëœ ë¹ˆë²ˆí•œ íŒ¨í„´: {len(frequent_itemsets)}ê°œ")

# 7. ì—°ê´€ê·œì¹™ ìƒì„±
rules_data = []
if len(frequent_itemsets) > 0:
    print(f"\nğŸ”— ì—°ê´€ê·œì¹™ ìƒì„± ì¤‘...")
    
    rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=0.4, num_itemsets=len(frequent_itemsets))
    
    if len(rules) > 0:
        rules = rules.sort_values('lift', ascending=False)
        print(f"âœ“ ë°œê²¬ëœ ì—°ê´€ê·œì¹™: {len(rules)}ê°œ")
        
        # ìƒìœ„ 15ê°œ ê·œì¹™ ì¶œë ¥
        print(f"\nğŸ† ê°•ë ¥í•œ ì—°ê´€ê·œì¹™ Top 15 (Lift ê¸°ì¤€):")
        for idx, row in rules.head(15).iterrows():
            antecedents = ', '.join(list(row['antecedents']))
            consequents = ', '.join(list(row['consequents']))
            print(f"  {antecedents:20} => {consequents:20}")
            print(f"    Sup: {row['support']:.3f} | Conf: {row['confidence']:.3f} | Lift: {row['lift']:.2f}")
        
        # JSON ì €ì¥ìš© ë°ì´í„°
        for idx, row in rules.iterrows():
            rules_data.append({
                'antecedents': list(row['antecedents']),
                'consequents': list(row['consequents']),
                'support': float(row['support']),
                'confidence': float(row['confidence']),
                'lift': float(row['lift'])
            })
        
        # ì €ì¥
        with open(RULES_JSON, 'w', encoding='utf-8') as f:
            json.dump({
                'timestamp': datetime.now().isoformat(),
                'total_transactions': len(transactions),
                'total_rules': len(rules),
                'rules': rules_data[:50]  # ìƒìœ„ 50ê°œ
            }, f, indent=2, ensure_ascii=False)
        
        print(f"âœ“ ì—°ê´€ê·œì¹™ ì €ì¥: {RULES_JSON}")

# 8. í‚¤ì›Œë“œ íŒ¨í„´ ë„¤íŠ¸ì›Œí¬
print(f"\nğŸ“ˆ í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ìƒì„± ì¤‘...")

keyword_cooccurrence = defaultdict(lambda: defaultdict(int))

for transaction in transactions:
    for i, kw1 in enumerate(transaction):
        for kw2 in transaction[i+1:]:
            keyword_cooccurrence[kw1][kw2] += 1
            keyword_cooccurrence[kw2][kw1] += 1

# ì—£ì§€ ìƒì„±
edges = []
for kw1, connections in keyword_cooccurrence.items():
    for kw2, count in connections.items():
        if count > len(transactions) * 0.01:  # 1% ì´ìƒ
            edges.append({
                'source': kw1,
                'target': kw2,
                'weight': count,
                'support': count / len(transactions)
            })

edges = sorted(edges, key=lambda x: x['weight'], reverse=True)[:30]

# ë…¸ë“œ ìƒì„±
nodes = [{'id': k, 'count': c, 'frequency': c / len(rules_df)} 
         for k, c in keyword_freq.most_common(30)]

with open(PATTERNS_JSON, 'w', encoding='utf-8') as f:
    json.dump({
        'timestamp': datetime.now().isoformat(),
        'nodes': nodes,
        'edges': edges
    }, f, indent=2, ensure_ascii=False)

print(f"âœ“ í‚¤ì›Œë“œ íŒ¨í„´ ì €ì¥: {PATTERNS_JSON}")

# 9. ê³ ê¸‰ ìƒ˜í”Œë§: MiniBatchKMeansë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
print(f"\nğŸ¯ ê³ ê¸‰ ìƒ˜í”Œë§ (ì „ì²´ ë°ì´í„°ì…‹)...")

print(f"  1) TF-IDF ë²¡í„°í™”...")
vectorizer = TfidfVectorizer(max_features=100, min_df=10, max_df=0.5)
tfidf_matrix = vectorizer.fit_transform(df['caption'].fillna(''))

print(f"  2) MiniBatch K-Means í´ëŸ¬ìŠ¤í„°ë§ (K=15)...")
n_clusters = 15
kmeans = MiniBatchKMeans(n_clusters=n_clusters, random_state=42, batch_size=1000, n_init=3)
df['cluster'] = kmeans.fit_predict(tfidf_matrix)

print(f"âœ“ í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ")

# í´ëŸ¬ìŠ¤í„° ë¶„í¬
cluster_dist = df['cluster'].value_counts().sort_index()
print(f"\nğŸ“Š í´ëŸ¬ìŠ¤í„° ë¶„í¬:")
for cid, count in cluster_dist.items():
    pct = count / len(df) * 100
    print(f"  í´ëŸ¬ìŠ¤í„° {cid:2d}: {count:6,}ê°œ ({pct:5.1f}%)")

# 10. ë‹¤ì–‘ì„± ê¸°ë°˜ ìƒ˜í”Œë§
print(f"\n  3) ë‹¤ì–‘ì„± ê¸°ë°˜ ìƒ˜í”Œë§ (50K)...")

TARGET_SAMPLES = 50000
samples_per_cluster = TARGET_SAMPLES // n_clusters

selected_indices = []

for cluster_id in range(n_clusters):
    cluster_df = df[df['cluster'] == cluster_id]
    n_select = min(samples_per_cluster, len(cluster_df))
    
    if n_select > 0:
        # ëœë¤ ìƒ˜í”Œë§ (ë©”ëª¨ë¦¬ ì ˆì•½)
        sampled = cluster_df.sample(n=n_select, random_state=42)
        selected_indices.extend(sampled.index.tolist())

# ë¶€ì¡±ë¶„ ì±„ìš°ê¸°
if len(selected_indices) < TARGET_SAMPLES:
    remaining = TARGET_SAMPLES - len(selected_indices)
    remaining_indices = list(set(range(len(df))) - set(selected_indices))
    additional = np.random.choice(remaining_indices, size=min(remaining, len(remaining_indices)), replace=False)
    selected_indices.extend(additional)

print(f"âœ“ ì„ íƒëœ ìƒ˜í”Œ: {len(selected_indices):,}ê°œ")

# 11. ê²°ê³¼ ì €ì¥
selected_df = df.loc[selected_indices]

output_df = pd.DataFrame({
    'video': selected_df['file_path'],
    'prompt': selected_df['caption']
})

advanced_csv = OUTPUT_DIR / 'train_metadata_50k_advanced.csv'
output_df.to_csv(advanced_csv, index=False)

# ìƒ˜í”Œë§ ê²°ê³¼ ì €ì¥
selected_cluster_dist = selected_df['cluster'].value_counts().sort_index()

sampling_results = {
    'timestamp': datetime.now().isoformat(),
    'method': 'diversity_based_clustering',
    'total_samples': len(selected_df),
    'n_clusters': n_clusters,
    'cluster_distribution': [
        {
            'cluster_id': int(cid),
            'count': int(count),
            'percentage': float(count / len(selected_df) * 100)
        }
        for cid, count in selected_cluster_dist.items()
    ]
}

with open(SAMPLING_JSON, 'w', encoding='utf-8') as f:
    json.dump(sampling_results, f, indent=2, ensure_ascii=False)

print(f"âœ“ ê³ ê¸‰ ìƒ˜í”Œë§ CSV: {advanced_csv}")
print(f"âœ“ ìƒ˜í”Œë§ ê²°ê³¼ JSON: {SAMPLING_JSON}")

print(f"\nâœ… ë¶„ì„ ì™„ë£Œ!")
print(f"\nğŸ“‹ ìš”ì•½:")
print(f"  - ì—°ê´€ê·œì¹™: {len(rules_data)}ê°œ")
print(f"  - í‚¤ì›Œë“œ ì—£ì§€: {len(edges)}ê°œ")
print(f"  - í´ëŸ¬ìŠ¤í„°: {n_clusters}ê°œ")
print(f"  - ì„ íƒëœ ìƒ˜í”Œ: {len(selected_df):,}ê°œ")
