#!/usr/bin/env python3
"""
í´ëŸ¬ìŠ¤í„°ë³„ ê³ ìœ  í‚¤ì›Œë“œ ë¶„ì„ (TF-IDF ê¸°ë°˜)
ê° í´ëŸ¬ìŠ¤í„°ë¥¼ í•˜ë‚˜ì˜ ë¬¸ì„œë¡œ ë³´ê³ , í´ëŸ¬ìŠ¤í„° ê°„ ì°¨ë³„í™”ë˜ëŠ” í‚¤ì›Œë“œ ì¶”ì¶œ
"""

import pandas as pd
import numpy as np
from pathlib import Path
import json
from collections import Counter
from datetime import datetime
from kiwipiepy import Kiwi
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import MiniBatchKMeans

INPUT_CSV = '/home/maiordba/projects/vision/Wan2.2/preprocessed_data/all_train.csv'
OUTPUT_DIR = Path('/home/maiordba/projects/vision/Wan2.2/data_quality_analysis')
SAMPLING_JSON = OUTPUT_DIR / 'advanced_sampling_results.json'

print("=" * 80)
print("í´ëŸ¬ìŠ¤í„°ë³„ ê³ ìœ /íŠ¹ì§• í‚¤ì›Œë“œ ì¶”ì¶œ")
print("=" * 80)

# Kiwi ì´ˆê¸°í™”
print(f"\nðŸ”§ Kiwi ì´ˆê¸°í™”...")
kiwi = Kiwi()

# ë°ì´í„° ë¡œë“œ
print(f"\nðŸ“‚ ë°ì´í„° ë¡œë”©...")
df = pd.read_csv(INPUT_CSV, usecols=['clip_id', 'file_path', 'caption'])
print(f"âœ“ {len(df):,}ê°œ ìƒ˜í”Œ")

# ëª…ì‚¬ ì¶”ì¶œ
def extract_nouns(text):
    if pd.isna(text):
        return []
    try:
        result = kiwi.analyze(str(text))
        if not result:
            return []
        nouns = []
        for token, pos, _, _ in result[0][0]:
            if pos in ('NNG', 'NNP') and len(token) >= 2 and not token.isdigit():
                nouns.append(token)
        return nouns
    except:
        return []

# í´ëŸ¬ìŠ¤í„°ë§
print(f"\nðŸŽ¯ í´ëŸ¬ìŠ¤í„°ë§...")
vectorizer = TfidfVectorizer(max_features=100, min_df=10, max_df=0.5)
tfidf_matrix = vectorizer.fit_transform(df['caption'].fillna(''))

n_clusters = 15
kmeans = MiniBatchKMeans(n_clusters=n_clusters, random_state=42, batch_size=1000, n_init=3)
df['cluster'] = kmeans.fit_predict(tfidf_matrix)

print(f"âœ“ ì™„ë£Œ")

# ìƒ˜í”Œë§í•˜ì—¬ ëª…ì‚¬ ì¶”ì¶œ (ì „ì²´ëŠ” ë„ˆë¬´ ëŠë¦¼)
print(f"\nðŸ” í´ëŸ¬ìŠ¤í„°ë³„ ëª…ì‚¬ ì¶”ì¶œ (20K ìƒ˜í”Œ)...")
sample_df = df.sample(n=min(20000, len(df)), random_state=42)
sample_df['nouns'] = sample_df['caption'].apply(extract_nouns)

# ê° í´ëŸ¬ìŠ¤í„°ë¥¼ í•˜ë‚˜ì˜ "ë¬¸ì„œ"ë¡œ ë§Œë“¤ê¸°
print(f"\nðŸ“„ í´ëŸ¬ìŠ¤í„°ë³„ ë¬¸ì„œ ìƒì„±...")
cluster_documents = []
for cluster_id in range(n_clusters):
    cluster_samples = sample_df[sample_df['cluster'] == cluster_id]
    # ëª¨ë“  ëª…ì‚¬ë¥¼ í•˜ë‚˜ì˜ ë¬¸ìžì—´ë¡œ
    all_nouns = []
    for nouns_list in cluster_samples['nouns']:
        all_nouns.extend(nouns_list)
    cluster_doc = ' '.join(all_nouns)
    cluster_documents.append(cluster_doc)

# í´ëŸ¬ìŠ¤í„° ê°„ TF-IDFë¡œ ê³ ìœ  í‚¤ì›Œë“œ ì°¾ê¸°
print(f"\nðŸ”¬ í´ëŸ¬ìŠ¤í„°ë³„ íŠ¹ì§• í‚¤ì›Œë“œ ì¶”ì¶œ (TF-IDF)...")
cluster_vectorizer = TfidfVectorizer(max_features=10)
cluster_tfidf = cluster_vectorizer.fit_transform(cluster_documents)

# ê° í´ëŸ¬ìŠ¤í„°ì˜ ìƒìœ„ TF-IDF í‚¤ì›Œë“œ
cluster_info = []
feature_names = cluster_vectorizer.get_feature_names_out()

for cluster_id in range(n_clusters):
    # ì´ í´ëŸ¬ìŠ¤í„°ì˜ TF-IDF ë²¡í„°
    tfidf_scores = cluster_tfidf[cluster_id].toarray()[0]
    
    # ìƒìœ„ 5ê°œ í‚¤ì›Œë“œ (TF-IDF ì ìˆ˜ ê¸°ì¤€)
    top_indices = tfidf_scores.argsort()[-5:][::-1]
    distinctive_keywords = [(feature_names[idx], tfidf_scores[idx]) for idx in top_indices if tfidf_scores[idx] > 0]
    
    total_in_cluster = len(df[df['cluster'] == cluster_id])
    
    # ì˜ë¯¸ìžˆëŠ” ì„¤ëª… ìƒì„±
    if distinctive_keywords:
        top_words = [kw for kw, score in distinctive_keywords[:3]]
        description = ', '.join(top_words)
        
        # í´ëŸ¬ìŠ¤í„° ì£¼ì œ ë ˆì´ë¸”ë§ (íœ´ë¦¬ìŠ¤í‹±)
        theme = "ì¼ë°˜"
        if any(w in top_words for w in ['ê±´ë¬¼', 'ë„ì‹œ', 'ê±´ì¶•', 'ê±°ë¦¬', 'ë„ë¡œ']):
            theme = "ë„ì‹œ/ê±´ì¶•"
        elif any(w in top_words for w in ['ìžì—°', 'í’ê²½', 'ì‚°', 'ë°”ë‹¤', 'í•˜ëŠ˜']):
            theme = "ìžì—°/í’ê²½"
        elif any(w in top_words for w in ['ì „í†µ', 'ë¬¸í™”', 'ì—­ì‚¬', 'ìœ ì ']):
            theme = "ì „í†µ/ë¬¸í™”"
        elif any(w in top_words for w in ['ì‚°ì—…', 'ê³µìž¥', 'í˜„ìž¥', 'ê±´ì„¤']):
            theme = "ì‚°ì—…/í˜„ìž¥"
        elif any(w in top_words for w in ['êµí†µ', 'ì°¨ëŸ‰', 'ë„ë¡œ', 'ì´ë™']):
            theme = "êµí†µ/ì´ë™"
        elif any(w in top_words for w in ['êµìœ¡', 'í•™êµ', 'í•™ìƒ', 'ìˆ˜ì—…']):
            theme = "êµìœ¡"
        elif any(w in top_words for w in ['ì‹¤ë‚´', 'ë‚´ë¶€', 'ë°©', 'ê³µê°„']):
            theme = "ì‹¤ë‚´ê³µê°„"
    else:
        description = "ê¸°íƒ€"
        theme = "ì¼ë°˜"
        distinctive_keywords = []
    
    cluster_info.append({
        'cluster_id': cluster_id,
        'size': total_in_cluster,
        'percentage': total_in_cluster / len(df) * 100,
        'theme': theme,
        'description': description,
        'top_keywords': [{'keyword': kw, 'score': float(score)} for kw, score in distinctive_keywords[:5]]
    })

# ê²°ê³¼ ì¶œë ¥
print(f"\nðŸ“Š í´ëŸ¬ìŠ¤í„°ë³„ ê³ ìœ  íŠ¹ì§•:")
for info in sorted(cluster_info, key=lambda x: x['size'], reverse=True):
    print(f"  í´ëŸ¬ìŠ¤í„° {info['cluster_id']:2d} [{info['theme']:12}]: {info['size']:6,}ê°œ ({info['percentage']:5.1f}%)")
    print(f"    íŠ¹ì§• í‚¤ì›Œë“œ: {info['description']}")

# ë‹¤ì–‘ì„± ìƒ˜í”Œë§
print(f"\nðŸŽ² ë‹¤ì–‘ì„± ê¸°ë°˜ ìƒ˜í”Œë§ (50K)...")
TARGET_SAMPLES = 50000
samples_per_cluster = TARGET_SAMPLES // n_clusters
selected_indices = []

for cluster_id in range(n_clusters):
    cluster_df = df[df['cluster'] == cluster_id]
    n_select = min(samples_per_cluster, len(cluster_df))
    if n_select > 0:
        sampled = cluster_df.sample(n=n_select, random_state=42)
        selected_indices.extend(sampled.index.tolist())

if len(selected_indices) < TARGET_SAMPLES:
    remaining = TARGET_SAMPLES - len(selected_indices)
    remaining_indices = list(set(range(len(df))) - set(selected_indices))
    additional = np.random.choice(remaining_indices, size=min(remaining, len(remaining_indices)), replace=False)
    selected_indices.extend(additional)

selected_df = df.loc[selected_indices]
selected_cluster_dist = selected_df['cluster'].value_counts().sort_index()

# ì €ìž¥
sampling_results = {
    'timestamp': datetime.now().isoformat(),
    'method': 'diversity_based_clustering_with_distinctive_keywords',
    'total_samples': len(selected_df),
    'n_clusters': n_clusters,
    'cluster_distribution': [
        {
            'cluster_id': int(cid),
            'count': int(count),
            'percentage': float(count / len(selected_df) * 100),
            'theme': cluster_info[cid]['theme'],
            'description': cluster_info[cid]['description'],
            'top_keywords': cluster_info[cid]['top_keywords']
        }
        for cid, count in selected_cluster_dist.items()
    ]
}

with open(SAMPLING_JSON, 'w', encoding='utf-8') as f:
    json.dump(sampling_results, f, indent=2, ensure_ascii=False)

print(f"âœ“ ì €ìž¥: {SAMPLING_JSON}")
print(f"\nâœ… ì™„ë£Œ! ê° í´ëŸ¬ìŠ¤í„°ê°€ ê³ ìœ í•œ íŠ¹ì§•ì„ ê°€ì§€ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.")
